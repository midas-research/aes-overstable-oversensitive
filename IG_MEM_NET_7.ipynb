{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4X6HAViVdJu"
      },
      "source": [
        "\n",
        "# SETUP AND DEPS\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-sQ5nW4RDV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b4325f1-b499-4ba5-8d6f-6d3e4b135bbf"
      },
      "source": [
        "!rm -rf calling-out-bluff/\n",
        "!git clone https://github.com/SwapnilDreams100/calling-out-bluff.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'calling-out-bluff'...\n",
            "remote: Enumerating objects: 3471, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 3471 (delta 3), reused 12 (delta 2), pack-reused 3455\u001b[K\n",
            "Receiving objects: 100% (3471/3471), 908.76 MiB | 37.70 MiB/s, done.\n",
            "Resolving deltas: 100% (711/711), done.\n",
            "Checking out files: 100% (3481/3481), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oJTBGu57Rm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40a76e2-c47c-418d-a8a8-0f097d20c744"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sX7AJ0Eyahx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3acb4ff2-68fd-43cc-a67c-9a6174eace3e"
      },
      "source": [
        "! pip install xhtml2pdf imgkit\n",
        "! sudo apt-get install wkhtmltopdf xvfb\n",
        "# !wget http://nlp.sta/nford.edu/data/glove.6B.zip\n",
        "! cp ./drive/My\\ Drive/glove.6B.300d.txt ./calling-out-bluff/Model5-MemoryNets/glove/\n",
        "! cp ./drive/My\\ Drive/glove.6B.300d.txt ./calling-out-bluff/Model5-MemoryNets/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xhtml2pdf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/a3/6d4c760bb08e7669f8c8a565ef62fe5a42d28aebf8dbc44476d9dd13782c/xhtml2pdf-0.2.5.tar.gz (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.1MB/s \n",
            "\u001b[?25hCollecting imgkit\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/b5/9870b626f34d02acd38fccb2f67260d5e92fc0c5e7061815feaab293130f/imgkit-1.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: html5lib>=1.0 in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf) (1.0.1)\n",
            "Collecting pyPdf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf) (7.1.2)\n",
            "Collecting reportlab>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/36/e0a6859c4fdab893c5ebb570e387fcf8dfe77d283650cd8422ef331cd892/reportlab-3.5.67-cp37-cp37m-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf) (1.15.0)\n",
            "Collecting python-bidi>=0.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/33/b0/f942d146a2f457233baaafd6bdf624eba8e0f665045b4abd69d1b62d097d/python_bidi-0.4.2-py2.py3-none-any.whl\n",
            "Collecting arabic-reshaper>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/47/27/7b9b824f5342d8ee180027333f2e15842ea36f5bc2d3d24a4e6bb31fb596/arabic_reshaper-2.1.3-py3-none-any.whl\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib>=1.0->xhtml2pdf) (0.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper>=2.1.0->xhtml2pdf) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from arabic-reshaper>=2.1.0->xhtml2pdf) (56.1.0)\n",
            "Building wheels for collected packages: xhtml2pdf, pyPdf2\n",
            "  Building wheel for xhtml2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xhtml2pdf: filename=xhtml2pdf-0.2.5-cp37-none-any.whl size=233671 sha256=249a8024335def40bf2934d145ff719d355813d725042d44883ab108e7344f52\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/0f/15/6f8008b18ca84c08c198445b465c8038f745e444d7251a8266\n",
            "  Building wheel for pyPdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyPdf2: filename=PyPDF2-1.26.0-cp37-none-any.whl size=61085 sha256=12689b2b753de7ac736911bd08adecb5e1664c0f8065208bd05ddb8d54c159b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "Successfully built xhtml2pdf pyPdf2\n",
            "Installing collected packages: pyPdf2, reportlab, python-bidi, arabic-reshaper, xhtml2pdf, imgkit\n",
            "Successfully installed arabic-reshaper-2.1.3 imgkit-1.2.2 pyPdf2-1.26.0 python-bidi-0.4.2 reportlab-3.5.67 xhtml2pdf-0.2.5\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  avahi-daemon bind9-host geoclue-2.0 geoip-database gstreamer1.0-plugins-base\n",
            "  iio-sensor-proxy libavahi-core7 libavahi-glib1 libbind9-160 libbrotli1\n",
            "  libcap2-bin libcdparanoia0 libdaemon0 libdns1100 libgeoclue-2-0 libgeoip1\n",
            "  libgl1-mesa-glx libgstreamer-plugins-base1.0-0 libgstreamer1.0-0 libhyphen0\n",
            "  libisc169 libisccc160 libisccfg160 liblwres160 libmbim-glib4 libmbim-proxy\n",
            "  libmm-glib0 libnl-genl-3-200 libnss-mdns liborc-0.4-0 libpam-cap\n",
            "  libqmi-glib5 libqmi-proxy libqt5positioning5 libqt5qml5 libqt5quick5\n",
            "  libqt5sensors5 libqt5svg5 libqt5webchannel5 libqt5webkit5 libvisual-0.4-0\n",
            "  libwoff1 modemmanager usb-modeswitch usb-modeswitch-data wpasupplicant\n",
            "Suggested packages:\n",
            "  avahi-autoipd gvfs geoip-bin libvisual-0.4-plugins gstreamer1.0-tools\n",
            "  avahi-autoipd | zeroconf qt5-qmltooling-plugins comgt wvdial wpagui\n",
            "  libengine-pkcs11-openssl\n",
            "The following NEW packages will be installed:\n",
            "  avahi-daemon bind9-host geoclue-2.0 geoip-database gstreamer1.0-plugins-base\n",
            "  iio-sensor-proxy libavahi-core7 libavahi-glib1 libbind9-160 libbrotli1\n",
            "  libcap2-bin libcdparanoia0 libdaemon0 libdns1100 libgeoclue-2-0 libgeoip1\n",
            "  libgl1-mesa-glx libgstreamer-plugins-base1.0-0 libgstreamer1.0-0 libhyphen0\n",
            "  libisc169 libisccc160 libisccfg160 liblwres160 libmbim-glib4 libmbim-proxy\n",
            "  libmm-glib0 libnl-genl-3-200 libnss-mdns liborc-0.4-0 libpam-cap\n",
            "  libqmi-glib5 libqmi-proxy libqt5positioning5 libqt5qml5 libqt5quick5\n",
            "  libqt5sensors5 libqt5svg5 libqt5webchannel5 libqt5webkit5 libvisual-0.4-0\n",
            "  libwoff1 modemmanager usb-modeswitch usb-modeswitch-data wkhtmltopdf\n",
            "  wpasupplicant xvfb\n",
            "0 upgraded, 48 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 24.8 MB of archives.\n",
            "After this operation, 99.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdaemon0 amd64 0.14-6 [16.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libqt5svg5 amd64 5.9.5-0ubuntu1 [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcap2-bin amd64 1:2.25-1.2 [20.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgstreamer1.0-0 amd64 1.14.5-0ubuntu1~18.04.2 [865 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 liborc-0.4-0 amd64 1:0.4.28-1 [137 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgstreamer-plugins-base1.0-0 amd64 1.14.5-0ubuntu1~18.04.2 [688 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhyphen0 amd64 2.8.8-5 [26.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libqt5positioning5 amd64 5.9.5+dfsg-0ubuntu2 [131 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libqt5qml5 amd64 5.9.5-0ubuntu1.1 [1,242 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libqt5quick5 amd64 5.9.5-0ubuntu1.1 [1,201 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libqt5sensors5 amd64 5.9.5-0ubuntu1 [113 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libqt5webchannel5 amd64 5.9.5-0ubuntu1 [47.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbrotli1 amd64 1.0.3-1ubuntu1.3 [262 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libwoff1 amd64 1.0.2-1build0.1 [43.0 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libqt5webkit5 amd64 5.212.0~alpha2-7ubuntu1 [11.8 MB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpam-cap amd64 1:2.25-1.2 [7,268 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libisc169 amd64 1:9.11.3+dfsg-1ubuntu1.15 [238 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgeoip1 amd64 1.6.12-1 [71.8 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdns1100 amd64 1:9.11.3+dfsg-1ubuntu1.15 [959 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libisccc160 amd64 1:9.11.3+dfsg-1ubuntu1.15 [17.9 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libisccfg160 amd64 1:9.11.3+dfsg-1ubuntu1.15 [48.5 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbind9-160 amd64 1:9.11.3+dfsg-1ubuntu1.15 [27.6 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 liblwres160 amd64 1:9.11.3+dfsg-1ubuntu1.15 [34.7 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 bind9-host amd64 1:9.11.3+dfsg-1ubuntu1.15 [53.5 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 geoip-database all 20180315-1 [2,090 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libavahi-core7 amd64 0.7-3.1ubuntu1.2 [81.1 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 avahi-daemon amd64 0.7-3.1ubuntu1.2 [62.3 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libavahi-glib1 amd64 0.7-3.1ubuntu1.2 [7,720 B]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgeoclue-2-0 amd64 2.4.7-1ubuntu1 [25.6 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmm-glib0 amd64 1.10.0-1~ubuntu18.04.2 [179 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 geoclue-2.0 amd64 2.4.7-1ubuntu1 [80.6 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcdparanoia0 amd64 3.10.2+debian-13 [46.7 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/main amd64 libvisual-0.4-0 amd64 0.4.0-11 [99.2 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gstreamer1.0-plugins-base amd64 1.14.5-0ubuntu1~18.04.2 [586 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/main amd64 iio-sensor-proxy amd64 2.4-2 [45.0 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-glx amd64 20.0.8-0ubuntu1~18.04.1 [5,532 B]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmbim-glib4 amd64 1.18.0-1~ubuntu18.04.1 [91.4 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmbim-proxy amd64 1.18.0-1~ubuntu18.04.1 [5,572 B]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnl-genl-3-200 amd64 3.2.29-0ubuntu3 [11.2 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnss-mdns amd64 0.10-8ubuntu1 [21.2 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libqmi-glib5 amd64 1.22.0-1.2~ubuntu18.04.1 [494 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libqmi-proxy amd64 1.22.0-1.2~ubuntu18.04.1 [5,632 B]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 modemmanager amd64 1.10.0-1~ubuntu18.04.2 [728 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 wpasupplicant amd64 2:2.6-15ubuntu2.8 [955 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/main amd64 usb-modeswitch-data all 20170806-2 [30.7 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/main amd64 usb-modeswitch amd64 2.5.2+repack0-2ubuntu1 [53.6 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/universe amd64 wkhtmltopdf amd64 0.12.4-1 [181 kB]\n",
            "Fetched 24.8 MB in 2s (15.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 48.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdaemon0:amd64.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libdaemon0_0.14-6_amd64.deb ...\n",
            "Unpacking libdaemon0:amd64 (0.14-6) ...\n",
            "Selecting previously unselected package libqt5svg5:amd64.\n",
            "Preparing to unpack .../01-libqt5svg5_5.9.5-0ubuntu1_amd64.deb ...\n",
            "Unpacking libqt5svg5:amd64 (5.9.5-0ubuntu1) ...\n",
            "Selecting previously unselected package libcap2-bin.\n",
            "Preparing to unpack .../02-libcap2-bin_1%3a2.25-1.2_amd64.deb ...\n",
            "Unpacking libcap2-bin (1:2.25-1.2) ...\n",
            "Selecting previously unselected package libgstreamer1.0-0:amd64.\n",
            "Preparing to unpack .../03-libgstreamer1.0-0_1.14.5-0ubuntu1~18.04.2_amd64.deb ...\n",
            "Unpacking libgstreamer1.0-0:amd64 (1.14.5-0ubuntu1~18.04.2) ...\n",
            "Selecting previously unselected package liborc-0.4-0:amd64.\n",
            "Preparing to unpack .../04-liborc-0.4-0_1%3a0.4.28-1_amd64.deb ...\n",
            "Unpacking liborc-0.4-0:amd64 (1:0.4.28-1) ...\n",
            "Selecting previously unselected package libgstreamer-plugins-base1.0-0:amd64.\n",
            "Preparing to unpack .../05-libgstreamer-plugins-base1.0-0_1.14.5-0ubuntu1~18.04.2_amd64.deb ...\n",
            "Unpacking libgstreamer-plugins-base1.0-0:amd64 (1.14.5-0ubuntu1~18.04.2) ...\n",
            "Selecting previously unselected package libhyphen0:amd64.\n",
            "Preparing to unpack .../06-libhyphen0_2.8.8-5_amd64.deb ...\n",
            "Unpacking libhyphen0:amd64 (2.8.8-5) ...\n",
            "Selecting previously unselected package libqt5positioning5:amd64.\n",
            "Preparing to unpack .../07-libqt5positioning5_5.9.5+dfsg-0ubuntu2_amd64.deb ...\n",
            "Unpacking libqt5positioning5:amd64 (5.9.5+dfsg-0ubuntu2) ...\n",
            "Selecting previously unselected package libqt5qml5:amd64.\n",
            "Preparing to unpack .../08-libqt5qml5_5.9.5-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking libqt5qml5:amd64 (5.9.5-0ubuntu1.1) ...\n",
            "Selecting previously unselected package libqt5quick5:amd64.\n",
            "Preparing to unpack .../09-libqt5quick5_5.9.5-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking libqt5quick5:amd64 (5.9.5-0ubuntu1.1) ...\n",
            "Selecting previously unselected package libqt5sensors5:amd64.\n",
            "Preparing to unpack .../10-libqt5sensors5_5.9.5-0ubuntu1_amd64.deb ...\n",
            "Unpacking libqt5sensors5:amd64 (5.9.5-0ubuntu1) ...\n",
            "Selecting previously unselected package libqt5webchannel5:amd64.\n",
            "Preparing to unpack .../11-libqt5webchannel5_5.9.5-0ubuntu1_amd64.deb ...\n",
            "Unpacking libqt5webchannel5:amd64 (5.9.5-0ubuntu1) ...\n",
            "Selecting previously unselected package libbrotli1:amd64.\n",
            "Preparing to unpack .../12-libbrotli1_1.0.3-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libbrotli1:amd64 (1.0.3-1ubuntu1.3) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../13-libwoff1_1.0.2-1build0.1_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build0.1) ...\n",
            "Selecting previously unselected package libqt5webkit5:amd64.\n",
            "Preparing to unpack .../14-libqt5webkit5_5.212.0~alpha2-7ubuntu1_amd64.deb ...\n",
            "Unpacking libqt5webkit5:amd64 (5.212.0~alpha2-7ubuntu1) ...\n",
            "Selecting previously unselected package libpam-cap:amd64.\n",
            "Preparing to unpack .../15-libpam-cap_1%3a2.25-1.2_amd64.deb ...\n",
            "Unpacking libpam-cap:amd64 (1:2.25-1.2) ...\n",
            "Selecting previously unselected package libisc169:amd64.\n",
            "Preparing to unpack .../16-libisc169_1%3a9.11.3+dfsg-1ubuntu1.15_amd64.deb ...\n",
            "Unpacking libisc169:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Selecting previously unselected package libgeoip1:amd64.\n",
            "Preparing to unpack .../17-libgeoip1_1.6.12-1_amd64.deb ...\n",
            "Unpacking libgeoip1:amd64 (1.6.12-1) ...\n",
            "Selecting previously unselected package libdns1100:amd64.\n",
            "Preparing to unpack .../18-libdns1100_1%3a9.11.3+dfsg-1ubuntu1.15_amd64.deb ...\n",
            "Unpacking libdns1100:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Selecting previously unselected package libisccc160:amd64.\n",
            "Preparing to unpack .../19-libisccc160_1%3a9.11.3+dfsg-1ubuntu1.15_amd64.deb ...\n",
            "Unpacking libisccc160:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Selecting previously unselected package libisccfg160:amd64.\n",
            "Preparing to unpack .../20-libisccfg160_1%3a9.11.3+dfsg-1ubuntu1.15_amd64.deb ...\n",
            "Unpacking libisccfg160:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Selecting previously unselected package libbind9-160:amd64.\n",
            "Preparing to unpack .../21-libbind9-160_1%3a9.11.3+dfsg-1ubuntu1.15_amd64.deb ...\n",
            "Unpacking libbind9-160:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Selecting previously unselected package liblwres160:amd64.\n",
            "Preparing to unpack .../22-liblwres160_1%3a9.11.3+dfsg-1ubuntu1.15_amd64.deb ...\n",
            "Unpacking liblwres160:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Selecting previously unselected package bind9-host.\n",
            "Preparing to unpack .../23-bind9-host_1%3a9.11.3+dfsg-1ubuntu1.15_amd64.deb ...\n",
            "Unpacking bind9-host (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Selecting previously unselected package geoip-database.\n",
            "Preparing to unpack .../24-geoip-database_20180315-1_all.deb ...\n",
            "Unpacking geoip-database (20180315-1) ...\n",
            "Selecting previously unselected package libavahi-core7:amd64.\n",
            "Preparing to unpack .../25-libavahi-core7_0.7-3.1ubuntu1.2_amd64.deb ...\n",
            "Unpacking libavahi-core7:amd64 (0.7-3.1ubuntu1.2) ...\n",
            "Selecting previously unselected package avahi-daemon.\n",
            "Preparing to unpack .../26-avahi-daemon_0.7-3.1ubuntu1.2_amd64.deb ...\n",
            "Unpacking avahi-daemon (0.7-3.1ubuntu1.2) ...\n",
            "Selecting previously unselected package libavahi-glib1:amd64.\n",
            "Preparing to unpack .../27-libavahi-glib1_0.7-3.1ubuntu1.2_amd64.deb ...\n",
            "Unpacking libavahi-glib1:amd64 (0.7-3.1ubuntu1.2) ...\n",
            "Selecting previously unselected package libgeoclue-2-0:amd64.\n",
            "Preparing to unpack .../28-libgeoclue-2-0_2.4.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgeoclue-2-0:amd64 (2.4.7-1ubuntu1) ...\n",
            "Selecting previously unselected package libmm-glib0:amd64.\n",
            "Preparing to unpack .../29-libmm-glib0_1.10.0-1~ubuntu18.04.2_amd64.deb ...\n",
            "Unpacking libmm-glib0:amd64 (1.10.0-1~ubuntu18.04.2) ...\n",
            "Selecting previously unselected package geoclue-2.0.\n",
            "Preparing to unpack .../30-geoclue-2.0_2.4.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking geoclue-2.0 (2.4.7-1ubuntu1) ...\n",
            "Selecting previously unselected package libcdparanoia0:amd64.\n",
            "Preparing to unpack .../31-libcdparanoia0_3.10.2+debian-13_amd64.deb ...\n",
            "Unpacking libcdparanoia0:amd64 (3.10.2+debian-13) ...\n",
            "Selecting previously unselected package libvisual-0.4-0:amd64.\n",
            "Preparing to unpack .../32-libvisual-0.4-0_0.4.0-11_amd64.deb ...\n",
            "Unpacking libvisual-0.4-0:amd64 (0.4.0-11) ...\n",
            "Selecting previously unselected package gstreamer1.0-plugins-base:amd64.\n",
            "Preparing to unpack .../33-gstreamer1.0-plugins-base_1.14.5-0ubuntu1~18.04.2_amd64.deb ...\n",
            "Unpacking gstreamer1.0-plugins-base:amd64 (1.14.5-0ubuntu1~18.04.2) ...\n",
            "Selecting previously unselected package iio-sensor-proxy.\n",
            "Preparing to unpack .../34-iio-sensor-proxy_2.4-2_amd64.deb ...\n",
            "Unpacking iio-sensor-proxy (2.4-2) ...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "Preparing to unpack .../35-libgl1-mesa-glx_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libmbim-glib4:amd64.\n",
            "Preparing to unpack .../36-libmbim-glib4_1.18.0-1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libmbim-glib4:amd64 (1.18.0-1~ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libmbim-proxy.\n",
            "Preparing to unpack .../37-libmbim-proxy_1.18.0-1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libmbim-proxy (1.18.0-1~ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libnl-genl-3-200:amd64.\n",
            "Preparing to unpack .../38-libnl-genl-3-200_3.2.29-0ubuntu3_amd64.deb ...\n",
            "Unpacking libnl-genl-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Selecting previously unselected package libnss-mdns:amd64.\n",
            "Preparing to unpack .../39-libnss-mdns_0.10-8ubuntu1_amd64.deb ...\n",
            "Unpacking libnss-mdns:amd64 (0.10-8ubuntu1) ...\n",
            "Selecting previously unselected package libqmi-glib5:amd64.\n",
            "Preparing to unpack .../40-libqmi-glib5_1.22.0-1.2~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libqmi-glib5:amd64 (1.22.0-1.2~ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libqmi-proxy.\n",
            "Preparing to unpack .../41-libqmi-proxy_1.22.0-1.2~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libqmi-proxy (1.22.0-1.2~ubuntu18.04.1) ...\n",
            "Selecting previously unselected package modemmanager.\n",
            "Preparing to unpack .../42-modemmanager_1.10.0-1~ubuntu18.04.2_amd64.deb ...\n",
            "Unpacking modemmanager (1.10.0-1~ubuntu18.04.2) ...\n",
            "Selecting previously unselected package wpasupplicant.\n",
            "Preparing to unpack .../43-wpasupplicant_2%3a2.6-15ubuntu2.8_amd64.deb ...\n",
            "Unpacking wpasupplicant (2:2.6-15ubuntu2.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../44-xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Selecting previously unselected package usb-modeswitch-data.\n",
            "Preparing to unpack .../45-usb-modeswitch-data_20170806-2_all.deb ...\n",
            "Unpacking usb-modeswitch-data (20170806-2) ...\n",
            "Selecting previously unselected package usb-modeswitch.\n",
            "Preparing to unpack .../46-usb-modeswitch_2.5.2+repack0-2ubuntu1_amd64.deb ...\n",
            "Unpacking usb-modeswitch (2.5.2+repack0-2ubuntu1) ...\n",
            "Selecting previously unselected package wkhtmltopdf.\n",
            "Preparing to unpack .../47-wkhtmltopdf_0.12.4-1_amd64.deb ...\n",
            "Unpacking wkhtmltopdf (0.12.4-1) ...\n",
            "Setting up libqt5qml5:amd64 (5.9.5-0ubuntu1.1) ...\n",
            "Setting up libbrotli1:amd64 (1.0.3-1ubuntu1.3) ...\n",
            "Setting up libqt5quick5:amd64 (5.9.5-0ubuntu1.1) ...\n",
            "Setting up libisc169:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Setting up libqt5sensors5:amd64 (5.9.5-0ubuntu1) ...\n",
            "Setting up libdaemon0:amd64 (0.14-6) ...\n",
            "Setting up libisccc160:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Setting up libmbim-glib4:amd64 (1.18.0-1~ubuntu18.04.1) ...\n",
            "Setting up libpam-cap:amd64 (1:2.25-1.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up iio-sensor-proxy (2.4-2) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build0.1) ...\n",
            "Setting up geoip-database (20180315-1) ...\n",
            "Setting up libcap2-bin (1:2.25-1.2) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up libnl-genl-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Setting up libgeoip1:amd64 (1.6.12-1) ...\n",
            "Setting up libcdparanoia0:amd64 (3.10.2+debian-13) ...\n",
            "Setting up libmbim-proxy (1.18.0-1~ubuntu18.04.1) ...\n",
            "Setting up usb-modeswitch-data (20170806-2) ...\n",
            "Setting up libhyphen0:amd64 (2.8.8-5) ...\n",
            "Setting up usb-modeswitch (2.5.2+repack0-2ubuntu1) ...\n",
            "Setting up libmm-glib0:amd64 (1.10.0-1~ubuntu18.04.2) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libqt5positioning5:amd64 (5.9.5+dfsg-0ubuntu2) ...\n",
            "Setting up libqt5svg5:amd64 (5.9.5-0ubuntu1) ...\n",
            "Setting up libvisual-0.4-0:amd64 (0.4.0-11) ...\n",
            "Setting up liborc-0.4-0:amd64 (1:0.4.28-1) ...\n",
            "Setting up libavahi-glib1:amd64 (0.7-3.1ubuntu1.2) ...\n",
            "Setting up libqt5webchannel5:amd64 (5.9.5-0ubuntu1) ...\n",
            "Setting up libdns1100:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Setting up liblwres160:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Setting up libavahi-core7:amd64 (0.7-3.1ubuntu1.2) ...\n",
            "Setting up libgstreamer1.0-0:amd64 (1.14.5-0ubuntu1~18.04.2) ...\n",
            "Setcap worked! gst-ptp-helper is not suid!\n",
            "Setting up wpasupplicant (2:2.6-15ubuntu2.8) ...\n",
            "Created symlink /etc/systemd/system/dbus-fi.w1.wpa_supplicant1.service → /lib/systemd/system/wpa_supplicant.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/wpa_supplicant.service → /lib/systemd/system/wpa_supplicant.service.\n",
            "Setting up libqmi-glib5:amd64 (1.22.0-1.2~ubuntu18.04.1) ...\n",
            "Setting up libqmi-proxy (1.22.0-1.2~ubuntu18.04.1) ...\n",
            "Setting up libisccfg160:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Setting up libgstreamer-plugins-base1.0-0:amd64 (1.14.5-0ubuntu1~18.04.2) ...\n",
            "Setting up gstreamer1.0-plugins-base:amd64 (1.14.5-0ubuntu1~18.04.2) ...\n",
            "Setting up libqt5webkit5:amd64 (5.212.0~alpha2-7ubuntu1) ...\n",
            "Setting up modemmanager (1.10.0-1~ubuntu18.04.2) ...\n",
            "Created symlink /etc/systemd/system/dbus-org.freedesktop.ModemManager1.service → /lib/systemd/system/ModemManager.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ModemManager.service → /lib/systemd/system/ModemManager.service.\n",
            "Setting up libbind9-160:amd64 (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Setting up bind9-host (1:9.11.3+dfsg-1ubuntu1.15) ...\n",
            "Setting up wkhtmltopdf (0.12.4-1) ...\n",
            "Setting up avahi-daemon (0.7-3.1ubuntu1.2) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of force-reload.\n",
            "Created symlink /etc/systemd/system/dbus-org.freedesktop.Avahi.service → /lib/systemd/system/avahi-daemon.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/avahi-daemon.service → /lib/systemd/system/avahi-daemon.service.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/avahi-daemon.socket → /lib/systemd/system/avahi-daemon.socket.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libnss-mdns:amd64 (0.10-8ubuntu1) ...\n",
            "First installation detected...\n",
            "Checking NSS setup...\n",
            "Setting up libgeoclue-2-0:amd64 (2.4.7-1ubuntu1) ...\n",
            "Setting up geoclue-2.0 (2.4.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for systemd (237-3ubuntu10.47) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "cp: cannot stat './drive/My Drive/glove.6B.300d.txt': No such file or directory\n",
            "cp: cannot stat './drive/My Drive/glove.6B.300d.txt': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi1zKL26LJ8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f764b66-0639-4cb0-e0d8-ab15199144c7"
      },
      "source": [
        "%cd calling-out-bluff/Model5-MemoryNets/\n",
        "%tensorflow_version 1.x\n",
        "import data_utils\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import imgkit\n",
        "options={'xvfb': ''}\n",
        "from qwk import quadratic_weighted_kappa\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "early_stop_count = 0\n",
        "max_step_count = 10\n",
        "is_regression = True\n",
        "gated_addressing = False\n",
        "essay_set_id = 7\n",
        "batch_size = 15\n",
        "embedding_size = 300\n",
        "feature_size = 100\n",
        "l2_lambda = 0.3\n",
        "hops = 3\n",
        "reader = 'bow' # gru may not work\n",
        "epochs = 100\n",
        "num_samples = 1\n",
        "num_tokens = 42\n",
        "test_batch_size = batch_size\n",
        "random_state = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/calling-out-bluff/Model5-MemoryNets\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTgVHKBOVzWK"
      },
      "source": [
        "# MODEL ARCH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaHPoIhEMmLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f965f50-5176-4ea9-9876-35f18767d26f"
      },
      "source": [
        "if is_regression:\n",
        "    from memn2n_kv_regression import MemN2N_KV\n",
        "else:\n",
        "    from memn2n_kv import MemN2N_KV\n",
        "# print flags info\n",
        "orig_stdout = sys.stdout\n",
        "timestamp = time.strftime(\"%b_%d_%Y_%H:%M:%S\", time.localtime())\n",
        "folder_name = '{}'.format(essay_set_id)\n",
        "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs/\", folder_name))\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "# save output to a file\n",
        "#f = file(out_dir+'/out.txt', 'w')\n",
        "#sys.stdout = f\n",
        "print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "# hyper-parameters end here\n",
        "training_path = 'training_set_rel3.tsv'\n",
        "essay_list, resolved_scores, essay_id = data_utils.load_training_data(training_path, essay_set_id)\n",
        "\n",
        "max_score = max(resolved_scores)\n",
        "min_score = min(resolved_scores)\n",
        "if essay_set_id == 7:\n",
        "    min_score, max_score = 0, 30\n",
        "elif essay_set_id == 8:\n",
        "    min_score, max_score = 0, 60\n",
        "\n",
        "print( 'max_score is {} \\t min_score is {}\\n'.format(max_score, min_score))\n",
        "with open(out_dir+'/params', 'a') as f:\n",
        "    f.write('max_score is {} \\t min_score is {} \\n'.format(max_score, min_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing to /content/calling-out-bluff/Model5-MemoryNets/runs/7\n",
            "\n",
            "max_score is 30 \t min_score is 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzizYq6tNYvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60b5618-4215-4370-8f4d-9d88f9bbd8e1"
      },
      "source": [
        "# if essay_set_id == 7:\n",
        "#     min_score, max_score = 0, 30\n",
        "# elif essay_set_id == 8:\n",
        "#     min_score, max_score = 0, 60\n",
        "\n",
        "# score_range = range(min_score, max_score+1)\n",
        "\n",
        "# #word_idx, _ = data_utils.build_vocab(essay_list, vocab_limit)\n",
        "\n",
        "# load glove\n",
        "import data_utils\n",
        "word_idx, word2vec = data_utils.load_glove(42, dim=300)\n",
        "\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "  pickle.dump( word_idx,f)\n",
        "\n",
        "# vocab_size = len(word_idx) + 1\n",
        "# # stat info on data set\n",
        "\n",
        "# sent_size_list = list(map(len, [essay for essay in essay_list]))\n",
        "# # print(\"sent size list\", sent_size_list)\n",
        "# max_sent_size = max(sent_size_list)\n",
        "# mean_sent_size = int(np.mean(sent_size_list))\n",
        "\n",
        "# print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
        "# with open(out_dir+'/params', 'a') as f:\n",
        "#     f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
        "\n",
        "# print( 'The length of score range is {}'.format(len(score_range)))\n",
        "# E = data_utils.vectorize_data(essay_list, word_idx, max_sent_size)\n",
        "# # print(vocab_size)\n",
        "# labeled_data = zip(E, resolved_scores, sent_size_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cf7810228371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# load glove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/calling-out-bluff/Model5-MemoryNets/data_utils.py\u001b[0m in \u001b[0;36mload_glove\u001b[0;34m(token_num, dim)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), \"glove/glove.\"+str(6)+\n\u001b[0;32m---> 79\u001b[0;31m                            \"B.\" + str(dim) + \"d.txt\"), encoding=\"utf-8\") as f:\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/calling-out-bluff/Model5-MemoryNets/glove/glove.6B.300d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg1RBn6wNueN"
      },
      "source": [
        "def train_step(m, e, s, ma):\n",
        "    start_time = time.time()\n",
        "    feed_dict = {\n",
        "        model._query: e,\n",
        "        model._memory_key: m,\n",
        "        model._score_encoding: s,\n",
        "        model._mem_attention_encoding: ma,\n",
        "        model.keep_prob: 0.9\n",
        "        #model.w_placeholder: word2vec\n",
        "    }\n",
        "    _, step, predict_op, cost = sess.run([train_op, global_step, model.predict_op, model.cost], feed_dict)\n",
        "    end_time = time.time()\n",
        "    time_spent = end_time - start_time\n",
        "    return predict_op, cost, time_spent\n",
        "\n",
        "def test_step(e, m):\n",
        "    feed_dict = {\n",
        "        model._query: e,\n",
        "        model._memory_key: m,\n",
        "        model.keep_prob: 1\n",
        "        #model.w_placeholder: word2vec\n",
        "    }\n",
        "    preds, mem_attention_probs = sess.run([model.predict_op, model.mem_attention_probs], feed_dict)\n",
        "    if is_regression:\n",
        "        preds = np.clip(np.round(preds), min_score, max_score)\n",
        "        return preds, mem_attention_probs\n",
        "    else:\n",
        "        return preds, mem_attention_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z3jOAx8N_2z"
      },
      "source": [
        "fold_count = 0\n",
        "kf = KFold(n_splits=5, random_state=random_state)\n",
        "best_kappa_scores = []\n",
        "for train_index, test_index in kf.split(essay_id):\n",
        "    early_stop_count = 0\n",
        "    fold_count += 1\n",
        "    if fold_count>=2:\n",
        "      break\n",
        "    trainE = []\n",
        "    testE = []\n",
        "    train_scores = []\n",
        "    test_scores = []\n",
        "    train_essay_id = []\n",
        "    test_essay_id = []\n",
        "\n",
        "    for ite in train_index:\n",
        "        trainE.append(E[ite])\n",
        "        train_scores.append(resolved_scores[ite])\n",
        "        train_essay_id.append(essay_id[ite])\n",
        "    for ite in test_index:\n",
        "        testE.append(E[ite])\n",
        "        test_scores.append(resolved_scores[ite])\n",
        "        test_essay_id.append(essay_id[ite])\n",
        "    \n",
        "    memory = []\n",
        "    memory_score = []\n",
        "    memory_sent_size = []\n",
        "    memory_essay_ids = []\n",
        "    # pick sampled essay for each score\n",
        "    for i in score_range:\n",
        "    # test point: limit the number of samples in memory for 8\n",
        "        for j in range(num_samples):\n",
        "            if i in train_scores:\n",
        "                score_idx = train_scores.index(i)\n",
        "                score = train_scores.pop(score_idx)\n",
        "                essay = trainE.pop(score_idx)\n",
        "                sent_size = sent_size_list.pop(score_idx)\n",
        "                memory.append(essay)\n",
        "                memory_score.append(score)\n",
        "                memory_essay_ids.append(train_essay_id.pop(score_idx))\n",
        "                memory_sent_size.append(sent_size)\n",
        "    memory_size = len(memory)\n",
        "    if is_regression:\n",
        "    # bad naming\n",
        "        train_scores_encoding = train_scores\n",
        "    else:\n",
        "        train_scores_encoding = list(map(lambda x: score_range.index(x), train_scores))\n",
        "    \n",
        "    # data size\n",
        "    n_train = len(trainE)\n",
        "    n_test = len(testE)\n",
        "\n",
        "    print( 'The size of training data: {}'.format(n_train))\n",
        "    print( 'The size of testing data: {}'.format(n_test))\n",
        "    with open(out_dir+'/params{}'.format(fold_count), 'a') as f:\n",
        "        f.write('The size of training data: {}\\n'.format(n_train))\n",
        "        f.write('The size of testing data: {}\\n'.format(n_test))\n",
        "        f.write('\\nEssay scores in memory:\\n{}'.format(memory_score))\n",
        "        f.write('\\nEssay ids in memory:\\n{}'.format(memory_essay_ids))\n",
        "        f.write('\\nEssay ids in training:\\n{}'.format(train_essay_id))\n",
        "        f.write('\\nEssay ids in testing:\\n{}'.format(test_essay_id))\n",
        "\n",
        "    batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
        "    batches = [(start, end) for start, end in batches]\n",
        "    print(batches)\n",
        "    x = 1\n",
        "    if x == 1:\n",
        "        with tf.Graph().as_default():\n",
        "            session_conf = tf.ConfigProto(\n",
        "                allow_soft_placement=True,\n",
        "                log_device_placement=False)\n",
        "\n",
        "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "            # decay learning rate\n",
        "            starter_learning_rate = 0.0001\n",
        "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
        "\n",
        "            # test point\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.1)\n",
        "            best_kappa_so_far = 0.0\n",
        "            with tf.Session(config=session_conf) as sess:\n",
        "                model = MemN2N_KV(batch_size, vocab_size, max_sent_size, max_sent_size, memory_size,\n",
        "                                  memory_size, embedding_size, len(score_range), feature_size, hops, reader, l2_lambda)\n",
        "                grads_and_vars = optimizer.compute_gradients(model.loss_op, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
        "                grads_and_vars = [(tf.clip_by_norm(g, 10.0), v)\n",
        "                                  for g, v in grads_and_vars if g is not None]\n",
        "                #grads_and_vars = [(add_gradient_noise(g, 1e-4), v) for g, v in grads_and_vars]\n",
        "                train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
        "                sess.run(tf.global_variables_initializer(), feed_dict={model.w_placeholder: word2vec})\n",
        "                saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "                for i in range(1, epochs+1):\n",
        "                    train_cost = 0\n",
        "                    total_time = 0\n",
        "                    np.random.shuffle(batches)\n",
        "                    for start, end in batches:\n",
        "                        e = trainE[start:end]\n",
        "                        s = train_scores_encoding[start:end]\n",
        "                        s_num = train_scores[start:end]\n",
        "                        #batched_memory = []\n",
        "                        # batch sized memory\n",
        "                        #for _ in range(len(e)):\n",
        "                        #    batched_memory.append(memory)\n",
        "                        mem_atten_encoding = []\n",
        "                        for ite in s_num:\n",
        "                            mem_encoding = np.zeros(memory_size)\n",
        "                            for j_idx, j in enumerate(memory_score):\n",
        "                                if j == ite:\n",
        "                                    mem_encoding[j_idx] = 1\n",
        "                            mem_atten_encoding.append(mem_encoding)\n",
        "                        batched_memory = [memory] * (end-start)\n",
        "                        _, cost, time_spent = train_step(batched_memory, e, s, mem_atten_encoding)\n",
        "                        total_time += time_spent\n",
        "                        train_cost += cost\n",
        "                    print( 'Finish epoch {}, total training cost is {}, time spent is {}'.format(i, train_cost, total_time))\n",
        "                    # evaluation\n",
        "                    if i % 5 == 0 or i == 200:\n",
        "                        # test on training data\n",
        "                        train_preds = []\n",
        "                        for start in range(0, n_train, test_batch_size):\n",
        "                            end = min(n_train, start+test_batch_size)\n",
        "\n",
        "                            #batched_memory = []\n",
        "                            #for _ in range(end-start):\n",
        "                            #    batched_memory.append(memory)\n",
        "                            batched_memory = [memory] * (end-start)\n",
        "    #                         print(\"BM\", len(batched_memory))\n",
        "                            preds, _ = test_step(trainE[start:end], batched_memory)\n",
        "                            if type(preds) is np.float32:\n",
        "                                train_preds.append(preds)\n",
        "                            else:\n",
        "                                for ite in preds:\n",
        "                                    train_preds.append(ite)\n",
        "                        if not is_regression:\n",
        "                            train_preds = np.add(train_preds, min_score)\n",
        "                        #train_kappp_score = kappa(train_scores, train_preds, 'quadratic')\n",
        "                        train_kappp_score = quadratic_weighted_kappa(\n",
        "                            train_scores, train_preds, min_score, max_score)\n",
        "                        # test on test data\n",
        "                        test_preds = []\n",
        "                        test_atten_probs = []\n",
        "                        for start in range(0, n_test, test_batch_size):\n",
        "                            end = min(n_test, start+test_batch_size)\n",
        "\n",
        "                            #batched_memory = []\n",
        "                            #for _ in range(end-start):\n",
        "                            #    batched_memory.append(memory)\n",
        "                            batched_memory = [memory] * (end-start)\n",
        "    #                         print(\"Test\", len(testE[start:end]))\n",
        "                            \n",
        "                            preds, mem_attention_probs = test_step(testE[start:end], batched_memory)\n",
        "\n",
        "\n",
        "                            # preds2, explanations = explain_step(testE[start:end], batched_memory)\n",
        "                            # print(preds2, len(explanations), (explanations[0]).shape)\n",
        "                            if type(preds) is np.float32:\n",
        "                                test_preds.append(preds)\n",
        "                            else:\n",
        "                                for ite in preds:\n",
        "                                    test_preds.append(ite)\n",
        "                            for ite in mem_attention_probs:\n",
        "                                test_atten_probs.append(ite)\n",
        "                        if not is_regression:\n",
        "                            test_preds = np.add(test_preds, min_score)\n",
        "                        #test_kappp_score = kappa(test_scores, test_preds, 'quadratic')\n",
        "                        \n",
        "                        ##### STORE TEST DATA\n",
        "                        t = []\n",
        "                        m = []\n",
        "                        for start in range(0, n_test, test_batch_size):\n",
        "                            end = min(n_test, start+test_batch_size)\n",
        "                            batched_memory = [memory] * (end-start)\n",
        "                            t.append(testE[start:end])\n",
        "                            m.append(batched_memory)\n",
        "                        \n",
        "                        with open(str(essay_set_id)+'_'+str(fold_count)+'.pkl', 'wb') as f:\n",
        "                          pickle.dump((t,m), f)\n",
        "                        #########################\n",
        "                        test_kappp_score = quadratic_weighted_kappa(\n",
        "                            test_scores, test_preds, min_score, max_score)\n",
        "                        stat_dict = {'pred_score': test_preds}\n",
        "                        stat_df = pd.DataFrame(stat_dict)\n",
        "                        # save the model if it gets best kappa\n",
        "                        if(test_kappp_score > best_kappa_so_far):\n",
        "                            early_stop_count = 0\n",
        "                            best_kappa_so_far = test_kappp_score\n",
        "                            # stats on test\n",
        "                            # stat_df.to_csv(out_dir+'/predScore_'+str(fold_count))\n",
        "                            with open(out_dir+'/mem_atten', 'a') as f:\n",
        "                                for idx, ite in enumerate(test_essay_id):\n",
        "                                    f.write('{}\\n'.format(ite))\n",
        "                                    f.write('{}\\n'.format(test_atten_probs[idx]))\n",
        "                            saver.save(sess, out_dir+'/checkpoints_'+str(fold_count), global_step)\n",
        "                            \n",
        "                        else:\n",
        "                            early_stop_count += 1\n",
        "                        print(\"Training kappa score = {}\".format(train_kappp_score))\n",
        "                        print(\"Testing kappa score = {}\".format(test_kappp_score))\n",
        "                        with open(out_dir+'/eval_'.format(fold_count), 'a') as f:\n",
        "                            f.write(\"Training kappa score = {}\\n\".format(train_kappp_score))\n",
        "                            f.write(\"Testing kappa score = {}\\n\".format(test_kappp_score))\n",
        "                            f.write(\"Best Testing kappa score so far = {}\\n\".format(best_kappa_so_far))\n",
        "                            f.write('*'*10)\n",
        "                            f.write('\\n')\n",
        "                    if early_stop_count > max_step_count:\n",
        "                        break\n",
        "                best_kappa_scores.append(best_kappa_so_far)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1rl01419tWZ"
      },
      "source": [
        "# INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZBjozECi01B"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%cd /content/calling-out-bluff/Model5-MemoryNets/\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "# from xhtml2pdf import pisa\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzRVPEa4rZVe"
      },
      "source": [
        "fold_no = 1\n",
        "step = 15170\n",
        "essay_set_id = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K0SWvSVsa53"
      },
      "source": [
        "ATTRS_DIR = '/content/drive/My Drive/IG RESULTS/MEMORY NET/P'+str(essay_set_id)+'/'\n",
        "ATTRS_TSV = '/content/drive/My Drive/IG RESULTS/MEMORY NET/P'+str(essay_set_id)+'/attrs.tsv'\n",
        "\n",
        "def convert_html_to_pdf(source_html, output_filename):\n",
        "  # open output file for writing (truncated binary)\n",
        "  result_file = open(output_filename, \"w+b\")\n",
        "\n",
        "  # convert HTML to PDF\n",
        "  pisa_status = pisa.CreatePDF(\n",
        "          source_html,                # the HTML to convert\n",
        "          dest=result_file)           # file handle to recieve result\n",
        "\n",
        "  # close output file\n",
        "  result_file.close()                 # close output file\n",
        "\n",
        "  # return False on success and True on errors\n",
        "  return pisa_status.err   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mXLl4KN6jaN"
      },
      "source": [
        "# GET DATA AND MEMEORY OF VALIDATION SET\n",
        "with open('/content/drive/My Drive/IG RESULTS/MEM MODELS/'+str(essay_set_id)+'_'+str(fold_no)+'.pkl', 'rb') as f:\n",
        "  (t,m,test_scores) = pickle.load(f)\n",
        "  \n",
        "# LOAD TRAINED MODEL\n",
        "saver = tf.train.import_meta_graph(\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step)+\".meta\")  \n",
        "sess=tf.Session()\n",
        "saver.restore(sess,\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMQtCdRBWOSh"
      },
      "source": [
        "# IG CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OfFy2AcJDt3"
      },
      "source": [
        "# INSPIRED BY : https://github.com/ankurtaly/Integrated-Gradients\n",
        "\n",
        "class integrated_gradients:\n",
        "  def __init__(self, graph, sess, min, tokenizer_file = 'tokenizer.pkl',batch_size = 20, num_reps = 20):\n",
        "    \n",
        "    self.graph = graph\n",
        "    self.sess = sess\n",
        "    w1 = self.graph.get_tensor_by_name(\"input/essay:0\")\n",
        "    w2 = self.graph.get_tensor_by_name(\"input/memory_key:0\")\n",
        "    w3 = self.graph.get_tensor_by_name(\"input/keep_prob:0\")\n",
        "    self.batch_size = batch_size\n",
        "    self.min = min\n",
        "    self.INPUT_TENSORS = [w1,w2,w3]\n",
        "    self.num_reps = num_reps\n",
        "\n",
        "    with open(tokenizer_file, 'rb') as f:\n",
        "      tokenizer = pickle.load(f)\n",
        "    self.reverse_word_map = dict(map(reversed, tokenizer.items()))\n",
        "      \n",
        "    self.OUTPUT_TENSOR =  self.graph.get_tensor_by_name('prediction/Squeeze:0')\n",
        "    self.PRED_TENSOR =  self.graph.get_tensor_by_name('prediction/Squeeze:0')\n",
        "    self.EMBEDDING_TENSOR =  self.graph.get_tensor_by_name('embedding_lookup/Identity:0')\n",
        "    GRADIENT_TENSOR = tf.gradients(self.OUTPUT_TENSOR, self.EMBEDDING_TENSOR)\n",
        "    self.get_gradients = {}\n",
        "    self.get_gradients[0] = GRADIENT_TENSOR\n",
        "  \n",
        "  def get_tensor(self, name):\n",
        "    return self.graph.get_tensor_by_name(name)\n",
        "\n",
        "  def generate_baseline(self, sample):\n",
        "    baseline = np.zeros(sample.shape)\n",
        "    return baseline\n",
        "\n",
        "  def _get_feed_dict(self, input_df):\n",
        "      feed = {}\n",
        "      for i, key in enumerate(self.INPUT_TENSORS):\n",
        "          feed[key.name] = [input_df[i]]\n",
        "      return feed\n",
        "\n",
        "  def _get_ig_error(self, integrated_gradients, baseline_prediction, prediction,\n",
        "                  debug=False):\n",
        "      sum_attributions = 0\n",
        "      sum_attributions += np.sum(integrated_gradients)\n",
        "\n",
        "      delta_prediction = prediction - baseline_prediction\n",
        "\n",
        "      error_percentage = \\\n",
        "          100 * (delta_prediction - sum_attributions) / delta_prediction\n",
        "      if debug:\n",
        "          print(f'prediction is {prediction}')\n",
        "          print(f'baseline_prediction is {baseline_prediction}')\n",
        "          print(f'delta_prediction is {delta_prediction}')\n",
        "          print(f'sum_attributions are {sum_attributions}')\n",
        "          print(f'Error percentage is {error_percentage}')\n",
        "\n",
        "      return error_percentage\n",
        "\n",
        "  def _get_scaled_inputs(self, input_val, baseline_val, num_reps):\n",
        "      list_scaled_embeddings = []\n",
        "      scaled_embeddings = \\\n",
        "          [baseline_val + (float(i) / (num_reps * self.batch_size - 1)) *\n",
        "          (input_val - baseline_val) for i in range(0, num_reps * self.batch_size)]\n",
        "\n",
        "      for i in range(num_reps):\n",
        "          list_scaled_embeddings.append(\n",
        "              np.array(scaled_embeddings[i * self.batch_size:i * self.batch_size +\n",
        "                                                        self.batch_size]))\n",
        "      return np.array(list_scaled_embeddings)\n",
        "\n",
        "  def _get_unscaled_inputs(self, input_val):\n",
        "      unscaled_embeddings = [input_val] * self.batch_size\n",
        "\n",
        "      return np.array(unscaled_embeddings)\n",
        "\n",
        "  def _calculate_integral(self, ig):\n",
        "      ig = (ig[:-1] + ig[1:]) / 2.0  # trapezoidal rule\n",
        "      integral = np.average(ig, axis=0)\n",
        "      return integral\n",
        "\n",
        "  def explain(self, x, memory, max_allowed_error=5, debug=False):\n",
        "      num_reps = self.num_reps\n",
        "      baseline = self.generate_baseline(np.array(x))\n",
        "      # baseline_memory = self.generate_baseline(np.array(memory))\n",
        "      inp = [x, memory, 1]\n",
        "      base = [baseline, memory, 1]\n",
        "      \n",
        "      # pred = self.predict(inp)\n",
        "      c =  0\n",
        "      # print('channel:', c)\n",
        "      \n",
        "      attributions, baseline_prediction, prediction = \\\n",
        "          self._compute_ig(inp, base, c, num_reps=num_reps) #### MAIN FUNC\n",
        "      if debug:\n",
        "        error_percentage = \\\n",
        "            self._get_ig_error(attributions, baseline_prediction, prediction,\n",
        "                          debug=debug)\n",
        "\n",
        "      # while abs(error_percentage) > max_allowed_error:\n",
        "      #     num_reps += 5\n",
        "      #     if debug:\n",
        "      #         print(f'Num reps is {num_reps}, abs error percentage is '\n",
        "      #               f'{error_percentage}')\n",
        "      #     integrated_gradients, baseline_prediction, prediction = \\\n",
        "      #         self._compute_ig(inp, base, c, num_reps=num_reps)\n",
        "      #     error_percentage = \\\n",
        "      #         self._get_ig_error(integrated_gradients, baseline_prediction,\n",
        "      #                       prediction, debug=debug)\n",
        "\n",
        "      igs = attributions.astype('float')\n",
        "      words = self.sequence_to_text(inp[0])\n",
        "      assert len(igs) == len(words)\n",
        "      # html_code = self.visualize_token_attrs(words , igs)\n",
        "      return igs, words\n",
        "  \n",
        "  def predict(self, inp):\n",
        "      pred = self.sess.run(self.PRED_TENSOR,\n",
        "                              self._get_feed_dict(inp))\n",
        "      pred = np.clip(np.round(pred), 0, 30)\n",
        "      return pred\n",
        "\n",
        "  def _get_feed_dict_batch(self, input_df):\n",
        "      feed = {}\n",
        "      for i, key in enumerate(self.INPUT_TENSORS):\n",
        "          feed[key.name] = input_df[i]\n",
        "      return feed\n",
        "\n",
        "  def predict_batch(self, inp):\n",
        "      pred = self.sess.run(self.PRED_TENSOR,\n",
        "                              self._get_feed_dict_batch(inp))\n",
        "      pred = [np.clip(np.round(x), 0, 30) for x in pred]\n",
        "      return pred\n",
        "  \n",
        "  def visualize_token_attrs(self, tokens, attrs):\n",
        "    def get_color(attr):\n",
        "        if attr > 0:\n",
        "            g = int(128*attr) + 127\n",
        "            b = 128 - int(64*attr)\n",
        "            r = 128 - int(64*attr)\n",
        "        else:\n",
        "            g = 128 + int(64*attr)\n",
        "            b = 128 + int(64*attr)\n",
        "            r = int(-128*attr) + 127\n",
        "        return r,g,b\n",
        "    import matplotlib as mpl\n",
        "    cmap='PiYG'\n",
        "    cmap_bound = np.abs(attrs).max()\n",
        "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
        "    cmap = mpl.cm.get_cmap(cmap)\n",
        "\n",
        "    # bound = max(abs(attrs.max()), abs(attrs.min()))\n",
        "    # attrs = attrs/bound\n",
        "    html_text = \"\"\n",
        "    html_text+=\"<html><body> <style type=\\\"text/css\\\"> p { display: inline-block;  width: 183pt;}</style> <p>\"\n",
        "    for i, tok in enumerate(tokens[:60]):\n",
        "        if tok is not None:\n",
        "          color = mpl.colors.rgb2hex(cmap(norm(attrs[i])))\n",
        "          html_text += \" <mark style='background-color:{}'>{}</mark>\".format(color, tok)\n",
        "    html_text+=\"</p></body></html>\"\n",
        "    return (html_text)\n",
        "  \n",
        "  def visualize_token_attrs_full(self, tokens, attrs):\n",
        "    def get_color(attr):\n",
        "        if attr > 0:\n",
        "            g = int(128*attr) + 127\n",
        "            b = 128 - int(64*attr)\n",
        "            r = 128 - int(64*attr)\n",
        "        else:\n",
        "            g = 128 + int(64*attr)\n",
        "            b = 128 + int(64*attr)\n",
        "            r = int(-128*attr) + 127\n",
        "        return r,g,b\n",
        "    import matplotlib as mpl\n",
        "    cmap='PiYG'\n",
        "    cmap_bound = np.abs(attrs).max()\n",
        "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
        "    cmap = mpl.cm.get_cmap(cmap)\n",
        "\n",
        "    # bound = max(abs(attrs.max()), abs(attrs.min()))\n",
        "    # attrs = attrs/bound\n",
        "    html_text = \"\"\n",
        "    html_text+=\"<html><body><p>\"\n",
        "    for i, tok in enumerate(tokens):\n",
        "        if tok is not None:\n",
        "          color = mpl.colors.rgb2hex(cmap(norm(attrs[i])))\n",
        "          html_text += \" <mark style='background-color:{}'>{}</mark>\".format(color, tok)\n",
        "    html_text+=\"</p></body></html>\"\n",
        "    return (html_text)\n",
        "\n",
        "  def sequence_to_text(self, list_of_indices):\n",
        "      words = [self.reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "      return words\n",
        "\n",
        "  def _compute_ig(self, inp, base, c, num_reps):\n",
        "      tensor_values = self.sess.run(self.EMBEDDING_TENSOR,\n",
        "                              self._get_feed_dict(inp))\n",
        "\n",
        "      tensor_baseline_values = self.sess.run(self.EMBEDDING_TENSOR,\n",
        "                              self._get_feed_dict(base))\n",
        "      \n",
        "      # print(tensor_values.shape, tensor_baseline_values.shape)\n",
        "      scaled_embeddings = self._get_scaled_inputs(tensor_values[0],\n",
        "                                            tensor_baseline_values[0],\n",
        "                                            num_reps)\n",
        "      \n",
        "      # print(scaled_embeddings) # num_reps x batch_size x emb_shape\n",
        "      scaled_input_feed = {}\n",
        "      \n",
        "      for i, key in enumerate(self.INPUT_TENSORS):\n",
        "          ui = self._get_unscaled_inputs(inp[i])\n",
        "          if 'input/keep_prob:0' in key.name: ### SCALAR VALUE\n",
        "            scaled_input_feed[ self.get_tensor(key.name) ] = 1.0\n",
        "          else:\n",
        "            scaled_input_feed[ self.get_tensor(key.name) ] = ui\n",
        "      \n",
        "      scores = []\n",
        "      path_gradients = []\n",
        "      for i in range(num_reps):\n",
        "          scaled_input_feed[self.EMBEDDING_TENSOR] = scaled_embeddings[i]\n",
        "            \n",
        "          path_gradients_rep, scores_rep = self.sess.run(\n",
        "              [self.get_gradients[c], self.OUTPUT_TENSOR], scaled_input_feed)\n",
        "          \n",
        "          path_gradients.append(path_gradients_rep[0])\n",
        "          scores.append(scores_rep)\n",
        "      \n",
        "      baseline_prediction = scores[0][0]  # first score is the baseline prediction\n",
        "      baseline_prediction = np.clip(np.round(baseline_prediction), 0, 30)\n",
        "      prediction = scores[-1][-1]  # last score is the input prediction\n",
        "      prediction = np.clip(np.round(prediction), 0, 30)\n",
        "      \n",
        "      # integrating the gradients and multiplying with the difference of the baseline and input.\n",
        "      ig = np.concatenate(path_gradients, axis=0)\n",
        "      integral = self._calculate_integral(ig)\n",
        "      igs = (tensor_values[0] - tensor_baseline_values[0]) * integral\n",
        "      igs = np.sum(igs, axis=-1)\n",
        "\n",
        "      return igs, baseline_prediction, prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCfP6voRM4K1"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avl9DXrwReJW"
      },
      "source": [
        "# ! cp /content/drive/My\\ Drive/IG\\ RESULTS/MEM\\ MODELS/tokenizer.pkl ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EWOL_LYQTlH"
      },
      "source": [
        "# saver = tf.train.import_meta_graph(\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step)+\".meta\")  \n",
        "# sess=tf.Session()\n",
        "# saver.restore(sess,\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step))\n",
        "\n",
        "# CREATE IG MODEL\n",
        "graph = tf.get_default_graph()\n",
        "IG = integrated_gradients(graph, sess, min = min_score, batch_size= 20, num_reps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPP4N3I7P2tc"
      },
      "source": [
        "# LOADING DATA methods\n",
        "\n",
        "import re\n",
        "import os as os\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def load_training_data(training_df):\n",
        "    resolved_score = training_df['label_orig']\n",
        "    essays = training_df['text']\n",
        "    essay_list = []\n",
        "    # turn an essay to a list of words\n",
        "    for idx, essay in essays.iteritems():\n",
        "        essay = clean_str(essay)\n",
        "        essay_list.append(tokenize(essay))\n",
        "    return essay_list, resolved_score.tolist()\n",
        "\n",
        "def tokenize(sent):\n",
        "    '''Return the tokens of a sentence including punctuation.\n",
        "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
        "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
        "    >>> tokenize('I don't know')\n",
        "    ['I', 'don', '\\'', 'know']\n",
        "    '''\n",
        "    return [x.strip() for x in re.split('(\\W+)', sent) if x.strip()]\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" ( \", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    return string.strip().lower()\n",
        "\n",
        "def build_vocab(sentences, vocab_limit):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary mapping from word to index based on the sentences.\n",
        "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
        "    \"\"\"\n",
        "    # Build vocabulary\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    print( 'Total size of vocab is {}'.format(len(word_counts.most_common())))\n",
        "    # Mapping from index to word\n",
        "    # vocabulary_inv = [x[0] for x in word_counts.most_common(vocab_limit)]\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common(vocab_limit)]\n",
        "    \n",
        "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i+1 for i, x in enumerate(vocabulary_inv)}\n",
        "    return [vocabulary, vocabulary_inv]\n",
        "\n",
        "# data is DataFrame\n",
        "def vectorize_data(data, word_idx, sentence_size):\n",
        "    E = []\n",
        "    for essay in data:\n",
        "        ls = max(0, sentence_size - len(essay))\n",
        "        wl = []\n",
        "        for w in essay:\n",
        "            if w in word_idx:\n",
        "                wl.append(word_idx[w])\n",
        "            else:\n",
        "                wl.append(0)\n",
        "        wl += [0]*ls\n",
        "        E.append(wl)\n",
        "    return E"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUG1RXCUXVt7"
      },
      "source": [
        "# Load adversarial data and compute attributions + statistics on SMALL DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy_p6YS5PSbm"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "names = ['song_beg', 'song_end', 'false_beg','false_end','normal','shuffle', 'syn','incomp_data']\n",
        "adv_data_list = {}\n",
        "for name in names:\n",
        "  adv_data = pd.read_csv('/content/drive/My Drive/IG RESULTS/'+str(essay_set_id)+'_'+name+'.csv')\n",
        "  adv_data_list[name] = adv_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6kjz1CCPWN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "07e6e98b-cc64-4f06-f131-e46e1e97adb9"
      },
      "source": [
        "E_list = {}\n",
        "M_list = {}\n",
        "for adv_data in adv_data_list.keys():\n",
        "  essay_list, resolved_scores = load_training_data(adv_data_list[adv_data])\n",
        "  E = data_utils.vectorize_data(essay_list, word_idx, len(t[0][0]))\n",
        "  E_list[adv_data]= E\n",
        "  M_list[adv_data] = m[0][:len(E)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-40bc4711b9ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mM_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0madv_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madv_data_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0messay_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolved_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_data_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madv_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0messay_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mE_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madv_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-915277c65b8c>\u001b[0m in \u001b[0;36mload_training_data\u001b[0;34m(training_df)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mresolved_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_orig'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0messays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0messay_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# turn an essay to a list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQdKagXdPQ5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57052159-d0d7-473c-9d84-cc18fa2ea5d9"
      },
      "source": [
        "e   = E_list['normal']\n",
        "mem = M_list['normal']\n",
        "inp = [e,mem,1.0]\n",
        "labels_orig = IG.predict_batch(inp)\n",
        "labels_orig = [int(x) for x in labels_orig]\n",
        "labels_orig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9, 18, 22, 18, 13, 14, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txVUyuycPRR_"
      },
      "source": [
        "from xhtml2pdf import pisa\n",
        "def convert_html_to_pdf(source_html, output_filename):\n",
        "  result_file = open(output_filename, \"w+b\")\n",
        "  pisa_status = pisa.CreatePDF(source_html, dest=result_file)          \n",
        "  result_file.close()\n",
        "\n",
        "def save_stats_add(diff, diff_attr, word_list, ratio,output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\nnew words in top 10%:  '+ ', '.join(word_list))\n",
        "  result.write('\\npercent of top words in added words:  '+ str(ratio))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_mod(diff, diff_attr, changed_no,output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\nno of words which changed attr:  '+ str(changed_no))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_gen(babel_total, babel_unattrib, output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ntop attributed words:  '+ str(babel_total))\n",
        "  result.write('\\ntop unattributed words:  '+ str(babel_unattrib))\n",
        "  result.close()\n",
        "\n",
        "def top_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    \n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, -k)[-k:]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "def bottom_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "\n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, k)[:k]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "def save_normal_attrs(data, memory, labels_orig, essay_type, is_small = True):\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "  a_total = []\n",
        "  w_total= [] \n",
        "  for i,essay in enumerate(data):\n",
        "    attrs, words= IG.explain(x = data[i][:659], memory= memory[0])\n",
        "    label_new = IG.predict([data[i][:659], memory[0],1.0])\n",
        "    if is_small:\n",
        "      html = IG.visualize_token_attrs(words, attrs)\n",
        "      convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(int(label_new))+'.pdf')\n",
        "    a_total.append(attrs)\n",
        "    w_total.append(words)\n",
        "\n",
        "  return a_total, w_total\n",
        "\n",
        "def subfinder(l, sl):\n",
        "    sll=len(sl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "        if l[ind:ind+sll]==sl:\n",
        "            return ind,ind+sll-1\n",
        "\n",
        "def save_attrs_pdf(data, data_normal, memory, labels_orig, essay_type, type_add = False, type_mod = False, type_gen = False):\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "  a,w = data_normal\n",
        "  \n",
        "  pattern_none = [None, None, None, None, None]\n",
        "  \n",
        "  babel_total = {}\n",
        "  babel_unattrib = {}\n",
        "  \n",
        "  for i,essay in enumerate(data):\n",
        "    attrs, words= IG.explain(x = data[i][:659], memory= memory[0])\n",
        "    labels_new = int(IG.predict([data[i][:659], memory[0], 1.0]))\n",
        "    \n",
        "    if type_add:\n",
        "        diff = labels_orig[i] - labels_new\n",
        "        \n",
        "        loc = subfinder(w[i], pattern_none)[0]\n",
        "        loc2 = subfinder(words, pattern_none)[0]\n",
        "          \n",
        "        if essay_type == 'song_beg' or essay_type =='false_beg' :\n",
        "          pattern = w[i][:5]\n",
        "          loc_patt = subfinder(words, pattern)[0]\n",
        "          new_w = words[:loc_patt]\n",
        "          new_a = attrs[:loc_patt]\n",
        "\n",
        "        elif essay_type == 'song_end' or essay_type =='false_end':\n",
        "          new_w = words[loc:loc2]\n",
        "          new_a = attrs[loc:loc2]\n",
        "\n",
        "        top_words_orig = top_k_attrs(w[i], a[i], k = int(0.1*loc))\n",
        "        top_words = top_k_attrs(words , attrs, k = int(0.1*loc2))\n",
        "\n",
        "        top_words_final = [x for x in top_words if x not in top_words_orig]\n",
        "        html = IG.visualize_token_attrs(words, attrs)\n",
        "        convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "        save_stats_add(diff, diff_attr, top_words_final, len(top_words_final)/len(new_w), dir+'stats_'+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "    \n",
        "    elif type_gen:\n",
        "        try:\n",
        "          loc = subfinder(words, pattern_none)[0] \n",
        "        except Exception as e:\n",
        "          loc = len(words)\n",
        "        attrs_sign= []\n",
        "        for x in attrs:\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "\n",
        "        top_words, top_words_signs = top_k_attrs(words, attrs, attrs_sign, k = int(0.1*loc))\n",
        "        \n",
        "        for j,x in enumerate(top_words):\n",
        "          if x in babel_total.keys():\n",
        "            if top_words_signs[j] == '+':\n",
        "              babel_total[x][0]+=1\n",
        "            else:\n",
        "              babel_total[x][1]+=1\n",
        "          else:\n",
        "            if top_words_signs[j] == '+':\n",
        "              babel_total[x] = [1,0]\n",
        "            else:\n",
        "              babel_total[x] = [0,1]\n",
        "         \n",
        "        attrs_abs = [abs(x) for x in attrs]\n",
        "        bottom_words = bottom_k_attrs(words[:loc], attrs_abs[:loc], k = int(0.1*loc))\n",
        "        \n",
        "        for j,x in enumerate(bottom_words):\n",
        "          if x in babel_unattrib.keys():\n",
        "            babel_unattrib[x]+= 1\n",
        "          else:\n",
        "            babel_unattrib[x] = 1\n",
        "\n",
        "        html = IG.visualize_token_attrs(words, attrs)\n",
        "        convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_new+0)+'.pdf')\n",
        "\n",
        "    elif type_mod:\n",
        "        diff = labels_orig[i] - int(labels_new)\n",
        "        diff_attr = sum(attrs) - sum(a[i])\n",
        "        \n",
        "        if essay_type == 'shuffle' or essay_type == 'syn':\n",
        "          loc = subfinder(w[i], pattern_none)[0]\n",
        "          attrs_abs=[]\n",
        "          attrs_sign =[]\n",
        "          for x in a[i]:\n",
        "            attrs_abs.append(abs(x))\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "\n",
        "          top_words_orig, token_signs_orig = top_k_attrs(w[i], attrs_abs, attrs_sign, k = int(0.1*loc))\n",
        "          \n",
        "          loc = subfinder(words, pattern_none)[0]\n",
        "          attrs_abs=[]\n",
        "          attrs_sign =[]\n",
        "          for x in attrs:\n",
        "            attrs_abs.append(abs(x))\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "          \n",
        "          top_words, token_signs = top_k_attrs(words, attrs_abs, attrs_sign, k = int(0.1*loc))\n",
        "          print(token_signs_orig, token_signs)\n",
        "          changed_count = 0\n",
        "\n",
        "          for orig_index,t in enumerate(top_words_orig):\n",
        "            try:\n",
        "              ind = top_words.index(t)\n",
        "            except Exception as e:\n",
        "              ind = -1\n",
        "\n",
        "            if ind!=-1:\n",
        "              if token_signs_orig[orig_index] != token_signs[ind]:\n",
        "                  changed_count+=1\n",
        "\n",
        "        html = IG.visualize_token_attrs(words, attrs)\n",
        "        convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "        save_stats_mod(diff, diff_attr, changed_count, dir+'stats_'+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "  \n",
        "  if type_gen:\n",
        "    del babel_unattrib[None]\n",
        "    save_stats_gen(babel_total, babel_unattrib, dir+'stats_word_attributions.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1nAXvxTzelp"
      },
      "source": [
        "from xhtml2pdf import pisa\n",
        "def convert_html_to_pdf(source_html, output_filename):\n",
        "  result_file = open(output_filename, \"w+b\")\n",
        "  pisa_status = pisa.CreatePDF(source_html, dest=result_file)          \n",
        "  result_file.close()\n",
        "\n",
        "def save_stats_add(diff, diff_attr, word_list, ratio,output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\nnew words in top 10%:  '+ ', '.join(word_list))\n",
        "  result.write('\\npercent of top words in added words:  '+ str(ratio))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_mod(diff, diff_attr, changed_no,output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\nno of words which changed attr:  '+ str(changed_no))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_gen(babel_total, babel_unattrib, output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ntop attributed words:  '+ str(babel_total))\n",
        "  result.write('\\ntop unattributed words:  '+ str(babel_unattrib))\n",
        "  result.close()\n",
        "\n",
        "def top_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    \n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, -k)[-k:]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "def bottom_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "\n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, k)[:k]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "def save_normal_attrs(data, memory, labels_orig, essay_type, is_small = True):\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "  a_total = []\n",
        "  w_total= [] \n",
        "  for i,essay in enumerate(data):\n",
        "    attrs, words= IG.explain(x = data[i][:659], memory= memory[0])\n",
        "    label_new = IG.predict([data[i][:659], memory[0],1.0])\n",
        "    if is_small:\n",
        "      html = IG.visualize_token_attrs(words, attrs)\n",
        "      html_full = IG.visualize_token_attrs_full(words, attrs)\n",
        "\n",
        "      # convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(int(label_new))+'.pdf')\n",
        "      imgkit.from_string(html,  dir+str(i)+'_'+str(labels_orig[i])+'_'+str(int(label_new))+'.png', options=options)\n",
        "      imgkit.from_string(html_full,  dir+str(i)+'_'+str(labels_orig[i])+'_'+str(int(label_new))+'_full.png', options=options)\n",
        "\n",
        "    a_total.append(attrs)\n",
        "    w_total.append(words)\n",
        "\n",
        "  return a_total, w_total\n",
        "\n",
        "def subfinder(l, sl):\n",
        "    sll=len(sl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "        if l[ind:ind+sll]==sl:\n",
        "            return ind,ind+sll-1\n",
        "\n",
        "def save_attrs_pdf(data, data_normal, memory, labels_orig, essay_type, type_add = False, type_mod = False, type_gen = False):\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "  a,w = data_normal\n",
        "  \n",
        "  pattern_none = [None, None, None, None, None]\n",
        "  \n",
        "  babel_total = {}\n",
        "  babel_unattrib = {}\n",
        "  \n",
        "  for i,essay in enumerate(data):\n",
        "    attrs, words= IG.explain(x = data[i][:659], memory= memory[0])\n",
        "    labels_new = int(IG.predict([data[i][:659], memory[0], 1.0]))\n",
        "    \n",
        "    html = IG.visualize_token_attrs(words, attrs)\n",
        "    html_full = IG.visualize_token_attrs_full(words, attrs)\n",
        "\n",
        "    if not type_gen:\n",
        "      convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "      imgkit.from_string(html,  dir+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.png', options=options)\n",
        "      imgkit.from_string(html_full,  dir+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'_full.png', options=options)\n",
        "    \n",
        "    else:\n",
        "      convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_new+0)+'.pdf')\n",
        "      imgkit.from_string(html,  dir+str(i)+'_'+str(labels_new+0)+'.png', options=options)\n",
        "      imgkit.from_string(html_full,  dir+str(i)+'_'+str(labels_new+0)+'_full.png', options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhEe9eXxxs8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39673481-8b2d-41d7-ca29-62681f8aa9cd"
      },
      "source": [
        "e_normal = E_list['normal']\n",
        "m_normal = M_list['normal']\n",
        "data_normal = save_normal_attrs(e_normal , m_normal, labels_orig, 'normal_new')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "[>                                                           ] 0%\r[==============================>                             ] 50%\r[============================================================] 100%\rRendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "IhwLbearesCF",
        "outputId": "333b94cb-b3f2-4166-c785-b997d0810128"
      },
      "source": [
        "for i in range(99999999999999999999999999999999999999):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6b0eb95e7958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m99999999999999999999999999999999999999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evvYqPWjc8kd"
      },
      "source": [
        "gc.collect()\n",
        "e_add_song = E_list['incomp_data']\n",
        "memory = M_list['normal']\n",
        "save_normal_attrs(e_add_song, memory, labels_orig, 'incomp_data_new')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjdy75fUxs_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867a2265-e4a5-4492-ce49-b03169a01930"
      },
      "source": [
        "# gc.collect()\n",
        "# e_add_song = E_list['song_beg']\n",
        "# memory = M_list['normal']\n",
        "# save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'song_beg', type_add=True)\n",
        "\n",
        "# gc.collect()\n",
        "# e_add_song = E_list['song_end']\n",
        "# memory = M_list['normal']\n",
        "# save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'song_end', type_add=True)\n",
        "\n",
        "gc.collect()\n",
        "e_add_song = E_list['false_beg']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'false_beg', type_add=True)\n",
        "\n",
        "# gc.collect()\n",
        "# e_add_song = E_list['false_end']\n",
        "# memory = M_list['normal']\n",
        "# save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'false_end', type_add=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "[>                                                           ] 0%\r[==============================>                             ] 50%\r[============================================================] 100%\rRendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eAPhWQVxtCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff00f29-3d16-424c-b942-8567c758a7bb"
      },
      "source": [
        "gc.collect()\n",
        "e_add_song = E_list['syn']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'syn_new', type_mod = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "[>                                                           ] 0%\r[==============================>                             ] 50%\r[============================================================] 100%\rRendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLs4eiD-xtFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8a9b51-871a-4ec9-a81b-b06f9bc466a5"
      },
      "source": [
        "gc.collect()\n",
        "e_add_song = E_list['shuffle']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'shuffle_new', type_mod = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "[>                                                           ] 0%\r[==============================>                             ] 50%\r[============================================================] 100%\rRendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUB-3Dv7yHrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ec09a9-712c-45dd-8275-c7e6f4f441ea"
      },
      "source": [
        "# Babel\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1CIEpiDmzLmJ6LMCVSOmCKw_eOg4ocuS4', dest_path='/content/AES.zip', unzip=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1CIEpiDmzLmJ6LMCVSOmCKw_eOg4ocuS4 into /content/AES.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcQVQCbsxtHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb19c0e-6bff-48b3-8697-633d5125bb1c"
      },
      "source": [
        "gc.collect()\n",
        "babel_data = pd.read_csv('/content/AES_testcases/prompt'+str(essay_set_id)+'/prompt 7 babel - Sheet1.csv', names= ['text'])\n",
        "babel_data['label_orig'] = min_score\n",
        "\n",
        "essay_list, resolved_scores = load_training_data(babel_data)\n",
        "E_babel = data_utils.vectorize_data(essay_list, word_idx, len(t[0][0]))\n",
        "\n",
        "e_add_song = E_babel\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'babel_new', type_gen = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "[>                                                           ] 0%\r[==============================>                             ] 50%\r[============================================================] 100%\rRendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n",
            "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "libpng warning: iCCP: known incorrect sRGB profile\n",
            "Loading page (1/2)\n",
            "Rendering (2/2)                                                    \n",
            "Done                                                               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5eSTdyn7Bnw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdf6QEvooF1A"
      },
      "source": [
        "# Load adversarial attributions + statistics on BIG DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rJMlb2bPkHb"
      },
      "source": [
        "ATTRS_DIR = '/content/drive/MyDrive/IG RESULTS/MEMORY NET/P7/'\n",
        "\n",
        "### NORMAL\n",
        "path = ATTRS_DIR + 'attrs_normal.tsv'\n",
        "w = []\n",
        "a = []\n",
        "\n",
        "with open(path, 'r') as f:\n",
        "  for line in f:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "        if word_attr !='done':\n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          attrs.append(float(attr))\n",
        "      question_tokens = question_tokens\n",
        "      attrs = attrs + [0]*(659 - len(attrs))\n",
        "\n",
        "      w.append(question_tokens)\n",
        "      a.append(attrs)\n",
        "# w,a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxh2JX0E7Bkb"
      },
      "source": [
        "def save_stats_add(diff, diff_attr, percent, output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\npercent of top words in added words:  '+ str(ratio))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_mod_shuffle(diff, diff_attr, changed_no,output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\nno of top words which changed attr:  '+ str(changed_no))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_mod_syn(diff, diff_attr, changed_top_no,changed_bottom_no, top_words, bottom_words, output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\n diff in scores:  '+str(diff))\n",
        "  result.write('\\n diff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\n no of top words which changed attr:  '+ str(changed_top_no))\n",
        "  result.write('\\n no of bottom words which changed attr:  '+ str(changed_bottom_no))\n",
        "  result.write('\\n top words which changed attr:  '+ str(top_words))\n",
        "  result.write('\\n bottom words which changed attr:  '+ str(bottom_words))\n",
        "  result.close()\n",
        "\n",
        "\n",
        "def subfinder(l, sl):\n",
        "    sll=len(sl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "        if l[ind:ind+sll]==sl:\n",
        "            return ind,ind+sll-1\n",
        "\n",
        "def top_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    \n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, -k)[-k:]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "def bottom_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "\n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, k)[:k]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "def save_big_attrs_pdf(memory, essay_type, type_add = False, type_mod = False, type_gen = False):\n",
        "  ATTRS_DIR = '/content/drive/MyDrive/IG RESULTS/MEMORY NET/P7/'\n",
        "\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "  pattern_none = [None, None, None, None, None]\n",
        "  \n",
        "  if type_add:\n",
        "    diff_array = [] \n",
        "    diff_attr_array = []\n",
        "    percent_array = []\n",
        "\n",
        "  if type_mod:\n",
        "    diff_array = []\n",
        "    diff_attr_array = []\n",
        "\n",
        "    changed_count_array = []\n",
        "\n",
        "    changed_count_top_array = []\n",
        "    changed_count_bottom_array = []\n",
        "    changed_top_words = {}\n",
        "    changed_bottom_words = {}\n",
        "\n",
        "  ### NORMAL\n",
        "  path = ATTRS_DIR + 'attrs_normal.tsv'\n",
        "  w = []\n",
        "  a = []\n",
        "\n",
        "  with open(path, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        question_attrs = line.split('\\t')[0]\n",
        "        question_tokens = []\n",
        "        attrs = []\n",
        "        for word_attr in question_attrs.split('||'): \n",
        "          if word_attr !='done':\n",
        "            word, attr = word_attr.split('|')\n",
        "            question_tokens.append(word)\n",
        "            attrs.append(float(attr))\n",
        "        question_tokens = question_tokens\n",
        "        attrs = attrs + [0]*(659 - len(attrs))\n",
        "\n",
        "        w.append(question_tokens)\n",
        "        a.append(attrs)\n",
        "  \n",
        "  labels_orig = []\n",
        "  for ws in w:\n",
        "    E = vectorize_data([ws], word_idx, 659)\n",
        "    lab = int(IG.predict([E[0], memory[0],1.0]))\n",
        "    labels_orig.append(lab)\n",
        "    \n",
        "  ### ADV\n",
        "  path = ATTRS_DIR+ 'attrs_'+essay_type+'.tsv'\n",
        "  data = []\n",
        "  with open(path, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        question_attrs = line.split('\\t')[0]\n",
        "        question_tokens = []\n",
        "        attrs = []\n",
        "        for word_attr in question_attrs.split('||'): \n",
        "          if word_attr !='done':\n",
        "            word, attr = word_attr.split('|')\n",
        "            question_tokens.append(word)\n",
        "            attrs.append(float(attr))\n",
        "        question_tokens = question_tokens\n",
        "        attrs = attrs + [0]*(659 - len(attrs))\n",
        "\n",
        "        data.append((attrs, question_tokens))\n",
        "        print(len(attrs))\n",
        "\n",
        "  for i,essay in enumerate(data[:-1]):\n",
        "    attrs, words= essay\n",
        "    E = vectorize_data([words], word_idx, 659)\n",
        "  \n",
        "    labels_new = int(IG.predict([E[0], memory[0],1.0]))\n",
        "    \n",
        "    try:\n",
        "      loc = subfinder(w[i], pattern_none)[0]    #norm\n",
        "    except Exception as e:\n",
        "      loc = len(w[i])\n",
        "    \n",
        "    try:\n",
        "      loc2 = subfinder(words, pattern_none)[0]  #adv\n",
        "    except Exception as e:\n",
        "      loc2 = len(words)\n",
        "    # print(words, w[i])\n",
        "    unpad_w = w[i][:loc]\n",
        "    unpad_a = a[i][:loc]\n",
        "    unpad_attrs = attrs[:loc2]\n",
        "    unpad_words = words[:loc2]  \n",
        "    \n",
        "    # print(len(unpad_a), len(unpad_w), loc)\n",
        "    # print(len(unpad_words), len(unpad_attrs), loc2)\n",
        "\n",
        "    if type_add:\n",
        "          \n",
        "        # print(loc, loc2, w[i])\n",
        "        if essay_type == 'song_beg' or essay_type =='false_beg' :\n",
        "          pattern = unpad_w[1:6]\n",
        "          loc_patt = subfinder(unpad_words, pattern)[0] \n",
        "          # print(loc_patt)\n",
        "          new_w = unpad_words[:loc_patt]\n",
        "          new_a = unpad_attrs[:loc_patt]\n",
        "          other_a = unpad_attrs[loc_patt:loc2]\n",
        "\n",
        "        # elif essay_type == 'song_end' or essay_type =='false_end':\n",
        "        #   new_w = words[loc:loc2]\n",
        "        #   new_a = attrs[loc:loc2]\n",
        "        #   other_a = attrs[:loc]\n",
        "        #   # print(len(new_w))\n",
        "        diff = abs(int(labels_new) - labels_orig[i])\n",
        "        diff_array.append(diff)\n",
        "\n",
        "        if new_w != []:\n",
        "          top_words_orig = top_k_attrs(unpad_w, unpad_a, k = int(0.2*loc))\n",
        "          top_words = top_k_attrs(unpad_words , unpad_attrs, k = int(0.2*loc2))\n",
        "\n",
        "          top_words_final = [x for x in top_words if x not in top_words_orig]\n",
        "\n",
        "          diff_attr_frac = []\n",
        "          for i_attrs in range(0,len(other_a), len(new_a)):\n",
        "            if i_attrs+len(new_a) < len(other_a):\n",
        "              diff_attr_frac.append(   sum(other_a[   i_attrs:i_attrs+len(new_a)  ])  )\n",
        "            else:\n",
        "              diff_attr_frac.append(   sum(other_a[   i_attrs:len(other_a)  ])  )\n",
        "              break\n",
        "\n",
        "          new_diff_frac = sum(diff_attr_frac) / len(diff_attr_frac)\n",
        "\n",
        "          diff_attr = sum(new_a)/new_diff_frac\n",
        "          diff_attr_array.append(diff_attr)\n",
        "\n",
        "          percent = len(top_words_final)/len(new_w)\n",
        "          percent_array.append(percent)\n",
        "    \n",
        "    elif type_mod:\n",
        "        \n",
        "        diff_attr = ((sum(unpad_attrs) - sum(unpad_a)) / sum(unpad_attrs)) *100\n",
        "        diff_attr_array.append(diff_attr)\n",
        "\n",
        "        diff = abs(int(labels_new) - labels_orig[i])\n",
        "        diff_array.append(diff)\n",
        "        ###### SYN\n",
        "\n",
        "        if essay_type == 'syn':\n",
        "          # normal\n",
        "\n",
        "          attrs_abs=[]\n",
        "          attrs_sign_orig =[]\n",
        "          for x in unpad_a:\n",
        "            attrs_abs.append(abs(x))\n",
        "            if x>0:\n",
        "              attrs_sign_orig.append('+')\n",
        "            else:\n",
        "              attrs_sign_orig.append('-')\n",
        "\n",
        "          top_words_orig, token_signs_orig = top_k_attrs(unpad_w, attrs_abs, attrs_sign_orig, k = int(0.2*loc))\n",
        "          bottom_words_orig, bottom_token_signs_orig = bottom_k_attrs(unpad_w, attrs_abs, attrs_sign_orig, k = int(0.2*loc))\n",
        "\n",
        "          attrs_abs=[]\n",
        "          attrs_sign =[]\n",
        "          for x in unpad_attrs:\n",
        "            attrs_abs.append(abs(x))\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "          \n",
        "          top_words, token_signs = top_k_attrs(unpad_words, attrs_abs, attrs_sign, k = int(0.2*loc2))\n",
        "          bottom_words, bottom_token_signs = bottom_k_attrs(unpad_words, attrs_abs, attrs_sign, k = int(0.2*loc2))\n",
        "\n",
        "          changed_count_top = 0\n",
        "          changed_count_bottom = 0\n",
        "          \n",
        "          # top    \n",
        "          for orig_index,t in enumerate(top_words_orig):\n",
        "            try:\n",
        "              ind = unpad_w.index(t)\n",
        "            except Exception as e:\n",
        "              ind = -1\n",
        "            \n",
        "            try:\n",
        "              ind_adv = unpad_words.index(t)\n",
        "            except Exception as e:\n",
        "              ind_adv = -1\n",
        "\n",
        "            if ind!=-1 or ind_adv!=-1:\n",
        "              \n",
        "              if attrs_sign[ind_adv] != attrs_sign_orig[ind]:\n",
        "                  changed_count_top+=1\n",
        "                  changed_top_words[t] = unpad_words[ind_adv]\n",
        "          \n",
        "          # bottom\n",
        "          for orig_index,t in enumerate(bottom_words_orig):\n",
        "            try:\n",
        "              ind = unpad_w.index(t)\n",
        "            except Exception as e:\n",
        "              ind = -1\n",
        "\n",
        "            try:\n",
        "              ind_adv = unpad_words.index(t)\n",
        "            except Exception as e:\n",
        "              ind_adv = -1\n",
        "\n",
        "            if ind!=-1 or ind_adv!=-1:\n",
        "              if attrs_sign[ind_adv] != attrs_sign_orig[ind]:\n",
        "                  changed_count_bottom+=1\n",
        "                  changed_bottom_words[t] = unpad_words[ind_adv]\n",
        "          \n",
        "          changed_count_top_array.append(changed_count_top/len(top_words_orig))\n",
        "          changed_count_bottom_array.append(changed_count_bottom/len(bottom_words_orig))\n",
        "\n",
        "        ###### SHUFFLE \n",
        "        if essay_type == 'shuffle':\n",
        "          # print(i, len(w[i]), w[i])\n",
        "\n",
        "          attrs_abs=[]\n",
        "          attrs_sign =[]\n",
        "          for x in unpad_a:\n",
        "            attrs_abs.append(abs(x))\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "\n",
        "          top_words_orig, token_signs_orig = top_k_attrs(unpad_w, attrs_abs, attrs_sign, k = int(0.2*loc))\n",
        "          \n",
        "          attrs_abs=[]\n",
        "          attrs_sign =[]\n",
        "          for x in unpad_attrs:\n",
        "            attrs_abs.append(abs(x))\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "          \n",
        "          top_words, token_signs = top_k_attrs(unpad_words, attrs_abs, attrs_sign, k = int(0.2*loc2))\n",
        "          \n",
        "          changed_count = 0\n",
        "\n",
        "          for orig_index,t in enumerate(top_words_orig):\n",
        "            try:\n",
        "              ind = top_words.index(t)\n",
        "            except Exception as e:\n",
        "              ind = -1\n",
        "\n",
        "            if ind!=-1:\n",
        "              if token_signs_orig[orig_index] != token_signs[ind]:\n",
        "                  changed_count+=1\n",
        "          changed_count_array.append(changed_count/len(top_words_orig))\n",
        "  \n",
        "  def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "  \n",
        "  if type_add:\n",
        "    save_stats_add(Average(diff_array), Average(diff_attr_array), Average(percent_array),       dir+'stats_'+essay_type+'.txt')\n",
        "\n",
        "  if type_mod and essay_type == 'syn':\n",
        "    save_stats_mod_syn(Average(diff_array), Average(diff_attr_array), Average(changed_count_top_array), \\\n",
        "                   Average(changed_count_bottom_array), set(changed_top_words),  set(changed_bottom_words), \\\n",
        "                   dir+'stats_'+essay_type+'.txt')\n",
        "\n",
        "  if type_mod and essay_type == 'shuffle':\n",
        "    save_stats_mod_shuffle(Average(diff_array), Average(diff_attr_array), Average(changed_count_array), dir+'stats_'+essay_type+'.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jyfrmUv7Bgz"
      },
      "source": [
        "save_big_attrs_pdf(m[0], 'song_beg', type_add=True)\n",
        "save_big_attrs_pdf(m[0], 'song_end', type_add=True)\n",
        "save_big_attrs_pdf(m[0], 'false_beg', type_add=True)\n",
        "save_big_attrs_pdf(m[0], 'false_end', type_add=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJXPclKb7Bdp"
      },
      "source": [
        "save_big_attrs_pdf(m[0], 'shuffle', type_mod=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bvx1dF87BZ9"
      },
      "source": [
        "save_big_attrs_pdf(m[0], 'syn', type_mod=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMDvDvVL7BWp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT40BNfb6_-R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_liPUYA6_7F"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZiCC6GsoO4m"
      },
      "source": [
        "# Compute other statistics on NORMAL ATTRIBUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amSkrLmqxtM2"
      },
      "source": [
        "# GET TOP AND BOTTOM ATTRIBUTED WORDS\n",
        "def top_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    \n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, -k)[-k:]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "# ATTRS_TSV = '/content/drive/My Drive/IG RESULTS/P1/MEMORY NET/attrs.tsv'\n",
        "lines = []\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "      lines.append(line)\n",
        "lines = list(set(lines))\n",
        "\n",
        "def get_counts_list_normal( top_k = 10, is_abs = False, is_sign = False, is_percent = None):\n",
        "  \n",
        "  essay_list = []\n",
        "  counts_list = []\n",
        "  signs_list = []\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          if is_abs:\n",
        "            attrs.append(abs(float(attr)))\n",
        "          else:\n",
        "            attrs.append(float(attr))\n",
        "\n",
        "      essay_list.append(question_tokens)\n",
        "      if is_percent!=None:\n",
        "        top_k = int(is_percent*len(question_tokens))\n",
        "  \n",
        "      if top_k == None:\n",
        "        k = len(question_tokens)\n",
        "      else:\n",
        "        k = min(top_k, len(question_tokens))\n",
        "      \n",
        "      if is_sign:\n",
        "        signs = []\n",
        "\n",
        "        for i in attrs:\n",
        "          if i>0:\n",
        "            signs.append('+')\n",
        "          else:\n",
        "            signs.append('-')\n",
        "      # get top k words by attribution \n",
        "      c_list, sign_list = top_k_attrs(question_tokens , attrs, signs,  k = k)\n",
        "      counts_list.extend(c_list)\n",
        "      signs_list.extend(sign_list)\n",
        "\n",
        "  return counts_list, signs_list, essay_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiHdLF7YxtQS"
      },
      "source": [
        "counts_list, signs_list, essay_list = get_counts_list_normal(is_percent = 0.1, is_sign = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFVq163m1aJH"
      },
      "source": [
        "signs_dict= {}\n",
        "frequent_attributions = Counter(counts_list).most_common(50)\n",
        "for i in frequent_attributions:\n",
        "  w = i[0]\n",
        "  for j,w2 in enumerate(counts_list):\n",
        "    if w ==w2 :\n",
        "      if w not in signs_dict.keys():\n",
        "        signs_dict[w]=[]\n",
        "      else:\n",
        "        signs_dict[w].append(signs_list[j])\n",
        "\n",
        "for k,v in signs_dict.items():\n",
        "  signs_dict[k] = Counter(v)\n",
        "\n",
        "with open(ATTRS_DIR+'NORMAL_word_importance.txt','w') as f:\n",
        "  f.write(str(signs_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwj3vd_a1aMY"
      },
      "source": [
        "###NORMAL UNATTRIBUTED STUFF:\n",
        "def bottom_k_attrs(tokens, attrs, k=None):\n",
        "    k = min(k, len(tokens))    \n",
        "    return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "def neg_k_attrs(tokens, attrs, k=None):\n",
        "    k = min(k, len(tokens))    \n",
        "    return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k] if attrs[i]<0 ])\n",
        "\n",
        "lines = []\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "      lines.append(line)\n",
        "lines = list(set(lines))\n",
        "\n",
        "def get_bottom_list_normal( top_k = 10, is_abs = True, is_percent = None, is_neg= False):\n",
        "  \n",
        "  counts_list = []\n",
        "  signs_list = []\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          if is_abs:\n",
        "            attrs.append(abs(float(attr)))\n",
        "          else:\n",
        "            attrs.append(float(attr))\n",
        "\n",
        "      if is_percent!=None:\n",
        "        top_k = int(is_percent*len(question_tokens))\n",
        "  \n",
        "      if top_k == None:\n",
        "        k = len(question_tokens)\n",
        "      else:\n",
        "        k = min(top_k, len(question_tokens))\n",
        "      \n",
        "      # get top k words by attribution \n",
        "      if is_neg:\n",
        "        c_list = neg_k_attrs(question_tokens , attrs, k = k)\n",
        "      else:\n",
        "        c_list = bottom_k_attrs(question_tokens , attrs, k = k)\n",
        "      counts_list.extend(c_list)\n",
        "      \n",
        "  return counts_list\n",
        "\n",
        "counts_list_unattrib = get_bottom_list_normal(is_percent = 0.1)\n",
        "counts_list_neg = get_bottom_list_normal(is_percent = 0.1, is_abs = False, is_neg = True)\n",
        "\n",
        "with open(ATTRS_DIR+'NORMAL_bottom_attributed_words.txt','w') as f:\n",
        "  f.write(str(Counter(counts_list_unattrib).most_common(50)))\n",
        "with open(ATTRS_DIR+'NORMAL_negative_attributed_words.txt','w') as f:\n",
        "  f.write(str(Counter(counts_list_neg).most_common(50)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghdNmz9-1aPU"
      },
      "source": [
        "def plot_and_save(curve_data, filename, x = 'num. words in vocab', y= 'relative accuracy', title= 'title'):\n",
        "  import matplotlib.pyplot as plt\n",
        "  OVERSTABILITY_CURVE_FILE = filename\n",
        "  plt.plot(list(curve_data.keys()), list(curve_data.values()))\n",
        "  # plt.xscale('symlog')\n",
        "  plt.xlabel(x, fontsize=19)\n",
        "  plt.ylabel(y, fontsize=19)\n",
        "  plt.title(title)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE, format='eps')\n",
        "  # plt.savefig(OVERSTABILITY_CURVE_FILE.replace('eps','png'), format='png')\n",
        "  # plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def plot_and_save_both(a,b, filename, x = 'num. words in vocab', y= 'relative accuracy', title= 'title'):\n",
        "  import matplotlib.pyplot as plt\n",
        "  OVERSTABILITY_CURVE_FILE = filename\n",
        "  plt.plot(a,b)\n",
        "  # plt.xscale('symlog')\n",
        "  ax = plt.gca()\n",
        "  ax.invert_xaxis()\n",
        "  plt.xlabel(x, fontsize=19)\n",
        "  plt.title(title)\n",
        "  plt.ylabel(y, fontsize=19)\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE, format='eps')\n",
        "  # plt.savefig(OVERSTABILITY_CURVE_FILE.replace('eps','png'), format='png')\n",
        "  # plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtsYwpez1aZS"
      },
      "source": [
        "### NORMAL ATTR - VARIATION PLOTS\n",
        "pattern_none = [None, None, None, None, None]\n",
        "data = e_normal\n",
        "memory = m_normal\n",
        "for i in range(len(data)):\n",
        "  attrs, words = IG.explain(data[i][:659], memory = memory[0])\n",
        "  loc = subfinder(words, pattern_none)[0]\n",
        "  attrs = attrs[:loc]\n",
        "  attrs = [abs(x) for x in attrs]\n",
        "  words = words[:loc]\n",
        "  s= {}\n",
        "  l = len(attrs)\n",
        "  for j in range(0, l, l//5):\n",
        "    s[(j)] = sum(attrs[j: j+ l//5])\n",
        "  \n",
        "  dir = ATTRS_DIR+'normal/'\n",
        "  plot_and_save(s, dir+str(i)+'_attrs_variation.txt', x= 'word number', y='absolute attribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZtwoaTZ1am9"
      },
      "source": [
        "with open('/content/drive/My Drive/IG RESULTS/MEM MODELS/tokenizer.pkl', 'rb') as f:\n",
        "    word_to_idx = pickle.load(f)\n",
        "\n",
        "idx_to_word = {v: k for k, v in word_to_idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I5OQZ2X1akh"
      },
      "source": [
        "lines = []\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "      lines.append(line)\n",
        "lines = list(set(lines))\n",
        "\n",
        "def get_essay_list(tsv_lines):\n",
        "  \n",
        "  essay_list = []\n",
        "  attrs_list = []\n",
        "  l = []\n",
        "  for line in tsv_lines:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          attrs.append(float(attr))\n",
        "      l.append(len(question_tokens))\n",
        "      question_tokens_idx = [word_to_idx[x] for x in question_tokens] + [0]*(659 - len(question_tokens))\n",
        "      essay_list.append(question_tokens_idx)\n",
        "      attrs_list.append(attrs)\n",
        "\n",
        "  return essay_list, attrs_list\n",
        "\n",
        "essay_list, attrs_list = get_essay_list(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qvssfBl1ah-"
      },
      "source": [
        "w1 = graph.get_tensor_by_name(\"input/essay:0\")\n",
        "w2 = graph.get_tensor_by_name(\"input/memory_key:0\")\n",
        "w3 = graph.get_tensor_by_name(\"input/keep_prob:0\")\n",
        "INPUT_TENSORS = [w1,w2,w3]\n",
        "PRED_TENSOR = graph.get_tensor_by_name('prediction/Squeeze:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i63Yvi_t1acy"
      },
      "source": [
        "mem_total = []\n",
        "for i in range(len(essay_list)):\n",
        "  mem_total.append(m[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd0ebeK12NOk"
      },
      "source": [
        "feed = {}\n",
        "pred_array_orig = []\n",
        "input_df = [np.array(essay_list), np.array(mem_total), 1]\n",
        "for i, key in enumerate(INPUT_TENSORS):\n",
        "    feed[key.name] = input_df[i]\n",
        "pred = sess.run(PRED_TENSOR,feed)\n",
        "pred_array_orig.extend(pred)\n",
        "pred_array_orig = [int(x) for x in pred_array_orig]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxd-LA6o2NMR"
      },
      "source": [
        "def npos(orig, new):\n",
        "  count = 0\n",
        "  for i in range(len(orig)):\n",
        "    if new[i]>orig[i]:\n",
        "      count+=1 \n",
        "  return (count/len(orig))*100\n",
        "\n",
        "def nneg(orig, new):\n",
        "  count = 0\n",
        "  for i in range(len(orig)):\n",
        "    if new[i]<orig[i]:\n",
        "      count+=1 \n",
        "  return (count/len(orig))*100\n",
        "\n",
        "def nsame(orig, new):\n",
        "  count = 0\n",
        "  for i in range(len(orig)):\n",
        "    if new[i]==orig[i]:\n",
        "      count+=1 \n",
        "  return (count/len(orig))*100\n",
        "\n",
        "def mu(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    s+=(orig[i] - new[i])\n",
        "  return (s/n)/30\n",
        "\n",
        "def absmu(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    s+=(orig[i] - new[i])\n",
        "  return (abs(s)/n)/30\n",
        "\n",
        "def sd(orig, new):\n",
        "  mu_val = mu(orig, new)\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    s+=(orig[i] - new[i] - mu_val)**2\n",
        "  return ((s/n)**(1/2))/30\n",
        "\n",
        "def mupos(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    if new[i]>orig[i]:\n",
        "      s+=(orig[i] - new[i])\n",
        "  return (s/n)/30\n",
        "\n",
        "def muneg(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    if orig[i] > new[i]:\n",
        "      s+=-(orig[i] - new[i])\n",
        "  return (s/n)/30\n",
        "\n",
        "def get_pred_stats(orig, new, filename, K):\n",
        "  b = ('NPOS',  npos(orig, new))\n",
        "  c = ('NNEG',  nneg(orig, new))\n",
        "  d = ('NSAME', nsame(orig, new))\n",
        "  e = ('MU',    mu(orig, new))\n",
        "  f = ('ABSMU', absmu(orig, new))\n",
        "  g = ('SD',    sd(orig, new))\n",
        "  h = ('MUPOS', mupos(orig, new))\n",
        "  i = ('MUNEG', muneg(orig, new))\n",
        "  with open(filename, 'a') as file:\n",
        "    file.write(str(K)+\"___\"+str([b,c,d,e,f,g,h,i])+ \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygfK61NopSK9"
      },
      "source": [
        "# from xhtml2pdf import pisa\n",
        "\n",
        "def convert_html_to_pdf(source_html, output_filename):\n",
        "  result_file = open(output_filename, \"w+b\")\n",
        "  pisa_status = pisa.CreatePDF(source_html, dest=result_file)          \n",
        "  result_file.close()\n",
        "\n",
        "def save_attrs(data, memory, K, essay_type):\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "  elif K ==1:\n",
        "    filelist = [ f for f in os.listdir(dir)]\n",
        "    for f in filelist:\n",
        "        os.remove(os.path.join(dir, f))\n",
        "\n",
        "  attrs, words= IG.explain(x = data[:659], memory= memory[0])\n",
        "  label_new = IG.predict([data[:659], memory[0],1.0])\n",
        "  html = IG.visualize_token_attrs(words, attrs)\n",
        "  # convert_html_to_pdf(html, dir+str(K)+'_'+str(int(label_new))+'.pdf')\n",
        "  imgkit.from_string(html,  dir+str(K)+'_'+str(label_new+0)+'.png', options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLEXCzFP2NKO"
      },
      "source": [
        "# TOP WORDS ADDITION GRAPH\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import random\n",
        "from random import randint\n",
        "\n",
        "import numpy as np\n",
        "def top_k_attrs(tokens, attrs,k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    return ([tokens[i] for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "random.seed(42)\n",
        "d = {}\n",
        "p_list = {}\n",
        "\n",
        "chosen_id = 15\n",
        "print(chosen_id)\n",
        "for K in range(0,6):\n",
        "  \n",
        "  percent = K*0.2\n",
        "  preds_new = []\n",
        "  new_essay_list = []\n",
        "  avg_len = 0\n",
        "  c_list_total = []\n",
        "\n",
        "  for id, essay in enumerate(essay_list):\n",
        "    \n",
        "    top_k = int(percent* len(attrs_list[id]))\n",
        "    attrs = attrs_list[id]\n",
        "    question_tokens = essay_list[id]\n",
        "    try:\n",
        "      c_list = top_k_attrs(question_tokens , attrs, k = top_k)\n",
        "    except Exception as e:\n",
        "      c_list = list(set(question_tokens))\n",
        "    if top_k==0:\n",
        "      c_list = []\n",
        "    c_list_total.extend(c_list)\n",
        "    new_essay = []\n",
        "    count = 0\n",
        "    for i in range(len(question_tokens)):  \n",
        "      if question_tokens[i] in c_list and count<top_k:\n",
        "        new_essay.append(question_tokens[i])\n",
        "        count+=1\n",
        "    \n",
        "    # print(len(new_essay)/  len(question_tokens[:len(attrs)])  )\n",
        "    new_essay = new_essay + [0]*(659 - len(new_essay))\n",
        "    if id == chosen_id and K>0:\n",
        "      save_attrs(new_essay, mem_total, K, essay_type='incomplete_data_new')\n",
        "      break\n",
        "    # new_essay_list.append(new_essay)\n",
        "    # input_df = [np.array([new_essay]), np.array([mem_total[0]]), 1]\n",
        "    # for i, key in enumerate(INPUT_TENSORS):\n",
        "    #     feed[key.name] = input_df[i]\n",
        "    # pred = sess.run(PRED_TENSOR,feed)\n",
        "    # preds_new.append(pred)\n",
        "    # print(pred_array_orig[id],pred)\n",
        "    # print(abs(pred - pred_array_orig[id]))\n",
        "    # if round(abs(round(pred) - pred_array_orig[id])) == 1:\n",
        "    #   if id not in p_list.keys():\n",
        "    #     p_list[id] = percent*100\n",
        "    #   else:\n",
        "    #     pass\n",
        "\n",
        "  # preds_new = [int(x) for x in preds_new]\n",
        "\n",
        "  # acc = cohen_kappa_score(preds_new, pred_array_orig, weights='quadratic')\n",
        "  # get_pred_stats(pred_array_orig, preds_new, ATTRS_DIR+'stats_top.txt', int(percent*100))\n",
        "  # d[percent*100] = acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMu5Z5GqWKog"
      },
      "source": [
        "l = p_list.values()\n",
        "sum(l)/len(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWWeHC38_J4I"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "font = {#'family' : 'normal',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 16}\n",
        "\n",
        "matplotlib.rc('font', **font)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jCMK8eT2NHw"
      },
      "source": [
        "d\n",
        "plot_and_save(d,ATTRS_DIR+'adding_top.eps', x = '% length of response', y ='relative QWK', title= 'iteratively adding words\\n(in order of importance)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIGWEb_41aT7"
      },
      "source": [
        "# BOTTOM WORDS REMOVAL GRAPH\n",
        "\n",
        "def bottom_k_attrs(tokens, attrs,k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    return ([tokens[i] for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "d2_keys = []\n",
        "d2_vals = []\n",
        "p_list={}\n",
        "for K in range(0,6):\n",
        "  percent = K*0.2\n",
        "  preds_new = []\n",
        "  new_essay_list = []\n",
        "  avg_len = 0\n",
        "  c_list_total = []\n",
        "  for id, essay in enumerate(essay_list):\n",
        "    top_k = int(percent* len(attrs_list[id]))\n",
        "    attrs = attrs_list[id]\n",
        "    question_tokens = essay_list[id]\n",
        "    attrs = [abs(x) for x in attrs]\n",
        "    try:\n",
        "      c_list = bottom_k_attrs(question_tokens , attrs, k = top_k)\n",
        "    except Exception as e:\n",
        "      c_list = list(set(question_tokens))\n",
        "    if top_k==0:\n",
        "      c_list = []\n",
        "\n",
        "    # c_list_total.extend(c_list)\n",
        "    new_essay = []\n",
        "    count = 0\n",
        "    for i in range(len(question_tokens[:len(attrs)])):  \n",
        "      if question_tokens[:len(attrs)][i] in c_list and count<top_k:\n",
        "        count+=1\n",
        "        pass\n",
        "      else:\n",
        "        new_essay.append( question_tokens[:len(attrs)][i])\n",
        "    # avg_len+=len(new_essay)\n",
        "    # print(len(new_essay)/  len(question_tokens[:len(attrs)]) , new_essay, question_tokens[:len(attrs)])\n",
        "    \n",
        "    new_essay = new_essay + [0]*(659 - len(new_essay))\n",
        "    new_essay_list.append(new_essay)\n",
        "\n",
        "    input_df = [np.array([new_essay]), np.array([mem_total[0]]), 1]\n",
        "    for i, key in enumerate(INPUT_TENSORS):\n",
        "        feed[key.name] = input_df[i]\n",
        "    pred = sess.run(PRED_TENSOR,feed)\n",
        "    # print(abs(pred - pred_array_orig[id]))\n",
        "    if round(abs(pred - pred_array_orig[id])) == 1:\n",
        "      if id not in p_list.keys():\n",
        "        p_list[id] = percent*100\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "  # avg_len/=len(new_essay_list)\n",
        "  # c_len = len(set(c_list_total))\n",
        "  feed = {}\n",
        "  input_df = [np.array(new_essay_list), np.array(mem_total), 1]\n",
        "  for i, key in enumerate(INPUT_TENSORS):\n",
        "      feed[key.name] = input_df[i]\n",
        "  pred = sess.run(PRED_TENSOR,feed)\n",
        "  preds_new.extend(pred)\n",
        "  preds_new = [int(x) for x in preds_new]\n",
        "  \n",
        "  acc = cohen_kappa_score(preds_new, pred_array_orig, weights='quadratic')\n",
        "  get_pred_stats(pred_array_orig, preds_new, ATTRS_DIR+'stats_bottom.txt', int(100-percent*100))\n",
        "  # print(percent*100, avg_len, acc)\n",
        "  d2_keys.append(100-percent*100)\n",
        "  d2_vals.append(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjKI2m2eZzQW"
      },
      "source": [
        "l = p_list.values()\n",
        "sum(l)/len(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MP0xdSNxtTL"
      },
      "source": [
        "plot_and_save_both(d2_keys, d2_vals, ATTRS_DIR+'removing_bottom.eps', x = '% length of response', y ='relative QWK', title= 'iteratively removing words\\n(in reverse order of importance)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a9Da0pg3KSX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1RDGLdwkOqi"
      },
      "source": [
        "GENERATE ATTRIBUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAklSYZU3KZn"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "def create_model_and_train(mem, attr_type = 'add'):\n",
        "  ATTRS_DIR = '/content/drive/MyDrive/IG RESULTS/'\n",
        "  ATTRS_TSV = ATTRS_DIR+'MEMORY NET/P7/attrs_'+attr_type+'.tsv'\n",
        "  \n",
        "  saver = tf.train.import_meta_graph(\"/content/drive/MyDrive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step)+\".meta\")  \n",
        "  sess=tf.Session()\n",
        "  saver.restore(sess,\"/content/drive/MyDrive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step))\n",
        "\n",
        "  graph = tf.get_default_graph()\n",
        "  IG = integrated_gradients(graph, sess, min = 0, batch_size= 20, num_reps=40)\n",
        "\n",
        "  data = pd.read_csv(ATTRS_DIR+'big_7_'+attr_type+'.csv')\n",
        "  essay_list, resolved_scores = load_training_data(data)\n",
        "  E = data_utils.vectorize_data(essay_list, word_idx, 659)\n",
        "  \n",
        "  with open(ATTRS_TSV, 'w') as outf:\n",
        "    ans = ''\n",
        "    for i,v in enumerate(E):\n",
        "        tsv_string = ''\n",
        "        attrs, words= IG.explain(x = E[i][:659], memory= mem)\n",
        "\n",
        "        question_attrs = []\n",
        "        for ind in range(len(words)):\n",
        "          if words[ind] != None and str(attrs[ind])!=None:\n",
        "            question_attrs.append(\n",
        "                '|'.join([ words[ind], str(attrs[ind]) ])\n",
        "                )\n",
        "        tsv_string = ['||'.join(question_attrs)]\n",
        "        ans += '\\t'.join(tsv_string) + '\\n'\n",
        "        del attrs, words, question_attrs, tsv_string\n",
        "        gc.collect()\n",
        "    \n",
        "    outf.write(ans)\n",
        "    outf.flush()\n",
        "    del ans\n",
        "    gc.collect()\n",
        "    outf.write('done')\n",
        "  print('DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jxb8o5PgcJq"
      },
      "source": [
        "attr_type = 'normal'\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\n",
        "p.start()\n",
        "p.join()\n",
        "p.terminate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdXJylpvJFX-"
      },
      "source": [
        "attr_type = 'song_beg'\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\n",
        "p.start()\n",
        "p.join()\n",
        "p.terminate()\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "attr_type = 'song_end'\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\n",
        "p.start()\n",
        "p.join()\n",
        "p.terminate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RPnLmXg-o7y"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eydr11oZJFVM"
      },
      "source": [
        "attr_type = 'false_beg'\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\n",
        "p.start()\n",
        "p.join()\n",
        "p.terminate()\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "attr_type = 'false_end'\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\n",
        "p.start()\n",
        "p.join()\n",
        "p.terminate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rd9AjZG-lbc"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nb-CqNG-lYo"
      },
      "source": [
        "attr_type = 'shuffle'\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\n",
        "p.start()\n",
        "p.join()\n",
        "p.terminate()\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "attr_type = 'syn'\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\n",
        "p.start()\n",
        "p.join()\n",
        "p.terminate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XNsh-O5-lVf"
      },
      "source": [
        "# SYNONYM AND SHUFFLING STATS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGGI1L9sSpv9"
      },
      "source": [
        "######### SYNONYM DIFF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vodzFdhCRZV9"
      },
      "source": [
        "data = pd.read_csv(ATTRS_DIR+'big_7_syn.csv')\n",
        "essay_list, resolved_scores = load_training_data(data)\n",
        "E = data_utils.vectorize_data(essay_list, word_idx, 659)\n",
        "\n",
        "\n",
        "pred_array_syn=[]\n",
        "for i,v in enumerate(E):\n",
        "  pred = IG.predict([E[i][:659], m[0][0],1.0])\n",
        "  pred_array_syn.append(int(pred))\n",
        "\n",
        "data = pd.read_csv(ATTRS_DIR+'big_7_normal.csv')\n",
        "essay_list, resolved_scores = load_training_data(data)\n",
        "E = data_utils.vectorize_data(essay_list, word_idx, 659)\n",
        "\n",
        "\n",
        "pred_array_normal=[]\n",
        "for i,v in enumerate(E):\n",
        "  pred = IG.predict([E[i][:659], m[0][0],1.0])\n",
        "  pred_array_normal.append(int(pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnfkxlkiRcEr"
      },
      "source": [
        "get_pred_stats(pred_array_normal, pred_array_syn, filename = ATTRS_DIR+'MEMORY NET/P7/stats_synonym.txt', K = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg2bLZWtSxCn"
      },
      "source": [
        "########## SHUFFLING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBZzK-_zSw_V"
      },
      "source": [
        "import math\n",
        "data = pd.read_csv(ATTRS_DIR+'big_7_shuffle.csv')\n",
        "essay_list, resolved_scores = load_training_data(data)\n",
        "E = data_utils.vectorize_data(essay_list, word_idx, 659)\n",
        "\n",
        "pred_array_shuffle=[]\n",
        "for i,v in enumerate(E):\n",
        "  pred = IG.predict([E[i][:659], m[0][0],1.0])\n",
        "  pred = np.clip(np.round(pred), 0, 30)\n",
        "  pred_array_shuffle.append((pred))\n",
        "\n",
        "data = pd.read_csv(ATTRS_DIR+'big_7_normal.csv')\n",
        "essay_list, resolved_scores = load_training_data(data)\n",
        "E = data_utils.vectorize_data(essay_list, word_idx, 659)\n",
        "\n",
        "pred_array_normal=[]\n",
        "for i,v in enumerate(E):\n",
        "  pred = IG.predict([E[i][:659], m[0][0],1.0])\n",
        "  pred = np.clip(np.round(pred), 0, 30)\n",
        "  pred_array_normal.append((pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTcVjnspSw8Y"
      },
      "source": [
        "get_pred_stats(pred_array_normal, pred_array_shuffle, filename = ATTRS_DIR+'MEMORY NET/P7/stats_shuffled.txt', K = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAPXKAtESw0r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}